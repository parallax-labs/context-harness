{"documents": [{"id": "0629f23f-83b3-4e34-ae50-cd5fb29ddaa9", "title": "api-design-standards.md", "source": "filesystem", "source_id": "api-design-standards.md", "body": "# API Design Standards\n\nAll public and internal APIs at Acme must follow these standards.\n\n---\n\n## RESTful API Conventions\n\n### URL Structure\n\n```\n/{version}/{resource}/{id}/{sub-resource}\n```\n\nExamples:\n- `GET /v1/orders` \u2014 list orders\n- `GET /v1/orders/123` \u2014 get order by ID\n- `POST /v1/orders` \u2014 create order\n- `PUT /v1/orders/123` \u2014 update order\n- `DELETE /v1/orders/123` \u2014 delete order\n- `GET /v1/orders/123/items` \u2014 list items for order\n\n### HTTP Methods\n\n| Method | Usage | Idempotent |\n|--------|-------|------------|\n| GET | Read resource(s) | Yes |\n| POST | Create resource | No |\n| PUT | Full update | Yes |\n| PATCH | Partial update | Yes |\n| DELETE | Remove resource | Yes |\n\n### Response Codes\n\n| Code | Usage |\n|------|-------|\n| 200 | Success |\n| 201 | Created (POST) |\n| 204 | No Content (DELETE) |\n| 400 | Bad Request \u2014 validation failed |\n| 401 | Unauthorized \u2014 missing/invalid token |\n| 403 | Forbidden \u2014 insufficient permissions |\n| 404 | Not Found |\n| 409 | Conflict \u2014 duplicate resource |\n| 422 | Unprocessable Entity \u2014 business rule violation |\n| 429 | Too Many Requests \u2014 rate limited |\n| 500 | Internal Server Error |\n| 503 | Service Unavailable |\n\n---\n\n## Pagination\n\nAll list endpoints must support cursor-based pagination:\n\n```json\n{\n  \"data\": [...],\n  \"pagination\": {\n    \"cursor\": \"eyJpZCI6MTIzfQ==\",\n    \"has_more\": true,\n    \"total_count\": 1547\n  }\n}\n```\n\nQuery parameters:\n- `cursor` \u2014 opaque cursor from previous response\n- `limit` \u2014 items per page (default: 20, max: 100)\n- `sort` \u2014 field to sort by (default: `created_at`)\n- `order` \u2014 `asc` or `desc` (default: `desc`)\n\n---\n\n## Error Format\n\nAll errors must follow this structure:\n\n```json\n{\n  \"error\": {\n    \"code\": \"validation_failed\",\n    \"message\": \"Human-readable error message\",\n    \"details\": [\n      {\n        \"field\": \"email\",\n        \"message\": \"must be a valid email address\",\n        \"code\": \"invalid_format\"\n      }\n    ],\n    \"request_id\": \"req_abc123\",\n    \"timestamp\": \"2025-10-15T14:30:00Z\"\n  }\n}\n```\n\n### Standard Error Codes\n\n- `validation_failed` \u2014 request body/params invalid\n- `not_found` \u2014 resource does not exist\n- `unauthorized` \u2014 authentication required\n- `forbidden` \u2014 insufficient permissions\n- `conflict` \u2014 resource already exists\n- `rate_limited` \u2014 too many requests\n- `internal_error` \u2014 unexpected server error\n\n---\n\n## Authentication\n\nAll APIs use JWT Bearer tokens issued by our OAuth2 server:\n\n```\nAuthorization: Bearer eyJhbGciOiJSUzI1NiIs...\n```\n\n### Token Scopes\n\n| Scope | Access |\n|-------|--------|\n| `read:orders` | Read order data |\n| `write:orders` | Create/update orders |\n| `admin:orders` | Delete orders, manage rules |\n| `read:users` | Read user profiles |\n| `admin:users` | Manage user accounts |\n\n### Service-to-Service Authentication\n\nInternal services use mTLS with short-lived certificates rotated every 24 hours. The service mesh (Istio) handles certificate management transparently.\n\n---\n\n## Rate Limiting\n\n| Tier | Limit | Window |\n|------|-------|--------|\n| Standard | 100 req/min | Per API key |\n| Premium | 1000 req/min | Per API key |\n| Internal | 10000 req/min | Per service identity |\n\nRate limit headers in every response:\n```\nX-RateLimit-Limit: 100\nX-RateLimit-Remaining: 87\nX-RateLimit-Reset: 1697376000\n```\n\n---\n\n## Versioning\n\n- URL-based versioning: `/v1/`, `/v2/`\n- Major version increments only for breaking changes\n- Deprecation: minimum 6-month sunset period with `Sunset` header\n- Breaking changes require ADR and cross-team review\n\n---\n\n## OpenAPI Specification\n\nEvery API must have an OpenAPI 3.1 spec committed to the repo:\n\n```\nservice-name/\n\u251c\u2500\u2500 api/\n\u2502   \u2514\u2500\u2500 openapi.yaml\n\u251c\u2500\u2500 src/\n\u2514\u2500\u2500 tests/\n    \u2514\u2500\u2500 api-contract/\n        \u2514\u2500\u2500 contract_test.rs\n```\n\nContract tests validate that the implementation matches the spec. CI blocks merge if specs diverge.\n\n", "updated_at": 1771713480}, {"id": "bedca66c-b818-462b-b173-e66d0a6d6e47", "title": "architecture-decisions.md", "source": "filesystem", "source_id": "architecture-decisions.md", "body": "# Architecture Decision Records\n\n## ADR-001: Microservices over Monolith\n\n**Status:** Accepted  \n**Date:** 2025-06-15  \n**Author:** Sarah Chen, Principal Engineer\n\n### Context\n\nOur monolithic application has grown to 2.3 million lines of code. Build times exceed 45 minutes. Teams are blocking each other on deployments. The blast radius of any change affects the entire system.\n\n### Decision\n\nWe will decompose the monolith into domain-aligned microservices over the next 18 months. Each bounded context will become an independently deployable service.\n\n### Consequences\n\n- Teams can deploy independently with reduced coordination overhead\n- We must invest in service mesh infrastructure (Istio)\n- Cross-service transactions require saga patterns\n- Operational complexity increases significantly \u2014 we need robust observability\n\n---\n\n## ADR-002: Event-Driven Architecture with Kafka\n\n**Status:** Accepted  \n**Date:** 2025-07-22  \n**Author:** Marcus Rivera, Staff Engineer\n\n### Context\n\nSynchronous REST calls between services create tight coupling and cascade failures. The order processing pipeline requires seven service calls in sequence, creating a 3-second latency budget.\n\n### Decision\n\nAdopt Apache Kafka as the backbone for asynchronous inter-service communication. Domain events will be published to topic partitions. Services consume events and maintain local read models.\n\n### Consequences\n\n- Decoupled services improve resilience \u2014 one service failure doesn't cascade\n- Eventual consistency requires careful handling in the UI layer\n- We need schema registry (Confluent) for event contract management\n- Engineers must understand event sourcing patterns\n\n---\n\n## ADR-003: PostgreSQL as Primary Datastore\n\n**Status:** Accepted  \n**Date:** 2025-08-10  \n**Author:** Priya Patel, Database Engineering Lead\n\n### Context\n\nWe evaluated PostgreSQL, CockroachDB, and DynamoDB for our primary datastore needs. Our workload is 70% reads, 30% writes with complex query patterns including full-text search and JSON operations.\n\n### Decision\n\nPostgreSQL 16 with logical replication for read replicas. We will use JSONB columns for flexible metadata and pg_trgm for fuzzy text search.\n\n### Consequences\n\n- Mature ecosystem with excellent tooling\n- Logical replication enables zero-downtime migrations\n- We must manage connection pooling carefully (PgBouncer)\n- Vertical scaling limits require careful schema design and partitioning strategy\n\n---\n\n## ADR-004: Rust for Performance-Critical Services\n\n**Status:** Accepted  \n**Date:** 2025-09-01  \n**Author:** Alex Kim, Platform Team Lead\n\n### Context\n\nOur data ingestion pipeline processes 50,000 events per second. The current Python implementation consumes 32GB of RAM and occasionally drops events during traffic spikes. Garbage collection pauses cause latency outliers at p99.\n\n### Decision\n\nRewrite the ingestion pipeline and matching engine in Rust. Use Tokio for async I/O and zero-copy deserialization with serde.\n\n### Consequences\n\n- 10x throughput improvement with 4x less memory\n- Compilation guarantees eliminate entire classes of runtime errors\n- Smaller talent pool \u2014 must invest in Rust training\n- Build times are longer than Go but the safety guarantees are worth it\n\n", "updated_at": 1771713408}, {"id": "c8b7a0ee-7694-4e2e-be9d-d98828af90b7", "title": "data-platform.md", "source": "filesystem", "source_id": "data-platform.md", "body": "# Data Platform Architecture\n\nOur data platform enables analytics, machine learning, and real-time data processing at scale.\n\n---\n\n## Architecture Overview\n\n```\nSources \u2192 Ingestion \u2192 Processing \u2192 Storage \u2192 Serving\n                                                 \u2193\n                                           ML Training\n                                                 \u2193\n                                           Model Serving\n```\n\n---\n\n## Data Ingestion\n\n### Batch Ingestion (dbt + Airflow)\n\nHourly and daily batch jobs extract data from operational databases into Snowflake:\n\n```sql\n-- dbt model: orders_enriched\nSELECT\n    o.id,\n    o.created_at,\n    o.total_amount_cents,\n    o.status,\n    u.segment,\n    u.lifetime_value_cents,\n    p.category,\n    p.brand\nFROM {{ ref('stg_orders') }} o\nJOIN {{ ref('stg_users') }} u ON o.user_id = u.id\nJOIN {{ ref('stg_products') }} p ON o.product_id = p.id\nWHERE o.created_at >= '{{ var(\"start_date\") }}'\n```\n\n### Stream Processing (Kafka + Flink)\n\nReal-time events flow through Kafka to Apache Flink for:\n- Fraud detection (sub-second decisions)\n- Real-time inventory updates\n- Live conversion funnel analytics\n- Personalization signal aggregation\n\n```java\nDataStream<OrderEvent> orders = env\n    .fromSource(kafkaSource, WatermarkStrategy.forMonotonousTimestamps(), \"orders\")\n    .keyBy(OrderEvent::getUserId)\n    .window(TumblingEventTimeWindows.of(Time.minutes(5)))\n    .aggregate(new OrderAggregator());\n```\n\n---\n\n## Data Warehouse (Snowflake)\n\n### Schema Organization\n\n| Schema | Purpose | Refresh |\n|--------|---------|---------|\n| `raw` | Unmodified source data | Continuous |\n| `staging` | Cleaned, typed, deduplicated | Hourly |\n| `marts` | Business-ready aggregations | Hourly |\n| `ml_features` | Feature store for ML models | Daily |\n\n### Data Contracts\n\nEvery table has a contract defined in YAML:\n\n```yaml\nmodel:\n  name: orders_enriched\n  description: \"Orders with user and product dimensions\"\n  columns:\n    - name: id\n      type: string\n      tests: [not_null, unique]\n    - name: total_amount_cents\n      type: integer\n      tests: [not_null, positive]\n    - name: created_at\n      type: timestamp\n      tests: [not_null]\n  freshness:\n    warn_after: 2 hours\n    error_after: 4 hours\n```\n\nBreaking contract changes require:\n1. ADR with justification\n2. 2-week migration window\n3. Downstream consumer notification\n4. Backward-compatible transition period\n\n---\n\n## Machine Learning Infrastructure\n\n### Feature Store\n\nWe use Feast for online and offline feature serving:\n\n```python\nfrom feast import FeatureStore\n\nstore = FeatureStore(\"feature_repo/\")\n\n# Online serving (low latency)\nfeatures = store.get_online_features(\n    features=[\"user_features:lifetime_value\", \"user_features:order_count\"],\n    entity_rows=[{\"user_id\": \"usr_123\"}]\n)\n\n# Offline training (batch)\ntraining_df = store.get_historical_features(\n    entity_df=entity_df,\n    features=[\"user_features:lifetime_value\", \"product_features:category\"]\n)\n```\n\n### Model Training Pipeline\n\n1. **Data preparation:** dbt transforms \u2192 Feast features\n2. **Training:** SageMaker training jobs with experiment tracking (MLflow)\n3. **Evaluation:** Automated evaluation against holdout set\n4. **Registry:** Model registered in MLflow with metrics and artifacts\n5. **Deployment:** Canary deployment via SageMaker endpoints\n\n### Model Serving\n\n| Use Case | Serving Pattern | Latency Target |\n|----------|----------------|----------------|\n| Fraud detection | Real-time (gRPC) | < 50ms |\n| Recommendations | Near-real-time (REST) | < 200ms |\n| Forecasting | Batch (Airflow) | N/A |\n| Search ranking | Real-time (REST) | < 100ms |\n\n---\n\n## Data Quality\n\n### Automated Checks\n\n- **Schema validation:** Great Expectations checks on every pipeline run\n- **Freshness monitoring:** Alerts if data is stale beyond SLA\n- **Volume anomaly detection:** Statistical checks for unexpected drops/spikes\n- **Cross-source reconciliation:** Daily counts compared across systems\n\n### Data Lineage\n\nWe track data lineage automatically via dbt and Airflow:\n\n```\nraw.stripe_charges\n    \u2192 staging.stg_payments\n        \u2192 marts.revenue_by_day\n            \u2192 Dashboard: \"Revenue Overview\"\n            \u2192 ML Model: \"Churn Prediction\"\n```\n\nLineage helps answer: \"If this source table changes, what dashboards and models are affected?\"\n\n---\n\n## Access Control\n\n| Role | Access | Approval |\n|------|--------|----------|\n| Analyst | Read marts schemas | Team lead |\n| Data Engineer | Read/write all schemas | Data lead |\n| ML Engineer | Read staging + ml_features | Data lead |\n| Service Account | Specific tables via policy | Data lead + Security |\n\nPII data requires additional approval from the Privacy team and is accessed via tokenized views only.\n\n", "updated_at": 1771713564}, {"id": "3d3a08e5-1553-4cdc-97a3-e212cbbcc79b", "title": "deployment-pipeline.md", "source": "filesystem", "source_id": "deployment-pipeline.md", "body": "# Deployment Pipeline\n\nOur CI/CD pipeline ensures safe, automated deployments from commit to production.\n\n---\n\n## Pipeline Stages\n\n```\nCommit \u2192 Build \u2192 Test \u2192 Security Scan \u2192 Deploy Staging \u2192 Canary \u2192 Production\n```\n\n### 1. Build (GitHub Actions)\n\nTriggered on every push to `main` and on PR creation.\n\n```yaml\njobs:\n  build:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n      - name: Build\n        run: cargo build --release\n      - name: Lint\n        run: cargo clippy -- -D warnings\n      - name: Format check\n        run: cargo fmt --check\n      - name: Upload artifact\n        uses: actions/upload-artifact@v4\n```\n\nBuild artifacts are Docker images pushed to our private ECR registry with the Git SHA as the tag.\n\n### 2. Test Suite\n\nThree test tiers run in parallel:\n\n| Tier | Duration | What |\n|------|----------|------|\n| Unit | ~2 min | Pure function tests, mocked dependencies |\n| Integration | ~8 min | Database, message queue, external service tests |\n| E2E | ~15 min | Full user flow through the UI |\n\n**Flaky test policy:** If a test fails intermittently more than 3 times in 30 days, it's quarantined and the owning team has 1 sprint to fix or delete it.\n\n### 3. Security Scanning\n\n- **SAST:** Semgrep rules for common vulnerabilities\n- **Dependency audit:** `cargo audit` + Snyk for transitive dependencies\n- **Container scan:** Trivy scans Docker images for known CVEs\n- **Secret detection:** TruffleHog prevents accidental credential commits\n\nCritical findings block deployment. High findings must be addressed within 1 sprint.\n\n### 4. Staging Deployment\n\nArgoCD watches the staging branch and auto-syncs:\n\n```yaml\napiVersion: argoproj.io/v1alpha1\nkind: Application\nmetadata:\n  name: order-service-staging\nspec:\n  destination:\n    namespace: staging\n    server: https://kubernetes.default.svc\n  source:\n    repoURL: https://github.com/acme/deployments\n    path: services/order-service/staging\n```\n\nStaging mirrors production topology at 1/10th scale. Synthetic traffic generators simulate real user patterns.\n\n### 5. Canary Deployment\n\nProduction deployments use a canary strategy:\n\n1. Deploy new version to 5% of pods\n2. Monitor error rate, latency p99, and business metrics for 15 minutes\n3. If metrics are healthy, ramp to 25% \u2192 50% \u2192 100% over 1 hour\n4. Automatic rollback if error rate increases >0.5% or p99 latency increases >20%\n\n```yaml\napiVersion: flagger.app/v1beta1\nkind: Canary\nspec:\n  analysis:\n    interval: 1m\n    threshold: 5\n    maxWeight: 50\n    stepWeight: 10\n  metrics:\n    - name: request-success-rate\n      thresholdRange:\n        min: 99.5\n    - name: request-duration\n      thresholdRange:\n        max: 500\n```\n\n---\n\n## Rollback Procedures\n\n### Automatic Rollback\n\nCanary deployments auto-rollback when metrics breach thresholds. No human intervention needed.\n\n### Manual Rollback\n\nFor post-deploy issues discovered after canary graduation:\n\n```bash\n# Rollback to previous version\nkubectl rollout undo deployment/order-service -n production\n\n# Or deploy a specific version\nargocd app set order-service --revision <git-sha>\nargocd app sync order-service\n```\n\n### Database Rollback\n\nDatabase migrations are forward-only. To \"rollback\" a migration:\n\n1. Create a new migration that reverts the schema change\n2. Ensure the application code handles both old and new schema\n3. Deploy the reverting migration\n4. Deploy the application rollback\n\n**Rule:** Never use `DROP COLUMN` or `ALTER COLUMN` in a migration without a 2-phase deploy strategy.\n\n---\n\n## Environment Promotion\n\n| Environment | Purpose | Data | Scale |\n|-------------|---------|------|-------|\n| Dev | Local development | Synthetic | Single node |\n| Staging | Integration testing | Anonymized production | 1/10th prod |\n| Production | Live traffic | Real | Full scale |\n\nFeature flags (LaunchDarkly) control feature availability independently of deployments. This decouples deploy from release.\n\n---\n\n## Deployment Schedule\n\n- **Staging:** Continuous (every merge to main)\n- **Production:** Business hours only (9 AM - 4 PM ET, Mon-Thu)\n- **Freeze periods:** 2 weeks before major product launches, last week of each quarter\n- **Emergency deploys:** Anytime with SEV-1 incident commander approval\n\n", "updated_at": 1771713508}, {"id": "2f0a6ba0-7a83-49db-bb82-49e411732669", "title": "incident-response.md", "source": "filesystem", "source_id": "incident-response.md", "body": "# Incident Response Playbook\n\n## Severity Levels\n\n### SEV-1: Critical Production Outage\n- **Definition:** Complete loss of service for all users, data loss risk, or security breach\n- **Response time:** 15 minutes\n- **Escalation:** VP Engineering + On-call SRE + affected team leads\n- **Communication:** Status page updated every 15 minutes, exec Slack channel, customer success notified\n\n### SEV-2: Major Degradation\n- **Definition:** Significant feature unavailable, performance degraded >50%, data integrity concerns\n- **Response time:** 30 minutes\n- **Escalation:** On-call SRE + team lead\n- **Communication:** Status page updated every 30 minutes, engineering Slack channel\n\n### SEV-3: Minor Issue\n- **Definition:** Non-critical feature affected, workaround available, isolated impact\n- **Response time:** 4 hours (business hours)\n- **Escalation:** Team on-call\n- **Communication:** Team Slack channel, tracked in Jira\n\n---\n\n## On-Call Rotation\n\nTeams rotate weekly, Monday to Monday. The on-call engineer must:\n\n1. Acknowledge pages within 5 minutes\n2. Have laptop and reliable internet access at all times\n3. Be within 15 minutes of a workstation\n4. Escalate if unable to triage within 30 minutes\n5. Hand off to next on-call with a written summary\n\n### Compensation\n- $500/week flat on-call stipend\n- Additional $200 for each SEV-1 incident handled\n- Comp time: 4 hours off for each overnight page\n\n---\n\n## Incident Timeline Template\n\n```\nINCIDENT: [Short description]\nSEVERITY: SEV-[1/2/3]\nSTARTED: [Timestamp UTC]\nDETECTED: [Timestamp UTC] \u2014 [How detected: alert/customer report/monitoring]\nACKNOWLEDGED: [Timestamp UTC] \u2014 [Who]\nMITIGATED: [Timestamp UTC] \u2014 [What action]\nRESOLVED: [Timestamp UTC]\nDURATION: [Total time]\n\nIMPACT:\n- Users affected: [number/percentage]\n- Revenue impact: [estimate]\n- Data loss: [yes/no, details]\n\nROOT CAUSE:\n[Description]\n\nTIMELINE:\n[HH:MM] \u2014 Event description\n[HH:MM] \u2014 Event description\n\nACTION ITEMS:\n- [ ] [Description] \u2014 Owner: [name] \u2014 Due: [date]\n```\n\n---\n\n## Post-Incident Review Process\n\nEvery SEV-1 and SEV-2 incident requires a blameless post-mortem within 48 hours.\n\n### Rules\n1. **Blameless culture** \u2014 Focus on systems, not individuals\n2. **5 Whys** \u2014 Dig into root causes, not symptoms\n3. **Action items must be tracked** \u2014 Each item gets a Jira ticket with an owner and due date\n4. **Share learnings** \u2014 Post-mortem document shared in #engineering and presented at weekly all-hands\n\n### Post-Mortem Template\n- Incident summary\n- Timeline of events\n- Root cause analysis (5 Whys)\n- What went well\n- What could be improved\n- Action items with owners and deadlines\n\n---\n\n## Common Runbooks\n\n### Database Connection Pool Exhaustion\n1. Check current connections: `SELECT count(*) FROM pg_stat_activity;`\n2. Identify long-running queries: `SELECT * FROM pg_stat_activity WHERE state = 'active' ORDER BY query_start;`\n3. Kill idle connections older than 10 minutes\n4. Verify PgBouncer pool settings\n5. Check for connection leaks in application logs\n\n### Kafka Consumer Lag\n1. Check consumer group lag: `kafka-consumer-groups --describe --group <group>`\n2. Verify consumer health in Grafana dashboard\n3. Check for poison messages in dead letter queue\n4. Scale consumer instances if processing throughput is the bottleneck\n5. Verify schema compatibility if deserialization errors are present\n\n### Memory Pressure on Kubernetes Pods\n1. Check pod memory usage: `kubectl top pods -n <namespace>`\n2. Review recent deployments for memory regression\n3. Check for memory leaks using heap profiler\n4. Adjust resource limits if growth is expected\n5. Consider horizontal pod autoscaler (HPA) configuration\n\n", "updated_at": 1771713432}, {"id": "6dc49faf-7443-4970-a68d-cb22c35d0148", "title": "observability-stack.md", "source": "filesystem", "source_id": "observability-stack.md", "body": "# Observability Stack\n\nOur observability strategy follows the three pillars: metrics, logs, and traces.\n\n---\n\n## Metrics (Datadog)\n\n### Key Dashboards\n\n| Dashboard | Purpose | Alert Threshold |\n|-----------|---------|-----------------|\n| Service Health | Request rate, error rate, latency | Error rate > 1%, p99 > 2s |\n| Infrastructure | CPU, memory, disk, network | CPU > 80%, memory > 85% |\n| Business KPIs | Orders/min, revenue, conversion | Orders drop > 20% |\n| Kafka | Consumer lag, partition balance | Lag > 10,000 messages |\n| Database | Connections, query latency, replication lag | Connections > 80%, lag > 1s |\n\n### Custom Metrics Convention\n\n```\nacme.<service>.<subsystem>.<metric>\n```\n\nExamples:\n- `acme.order_service.api.request_duration`\n- `acme.order_service.kafka.messages_processed`\n- `acme.order_service.db.query_duration`\n- `acme.order_service.cache.hit_ratio`\n\n### SLOs and Error Budgets\n\n| Service | Availability SLO | Latency SLO (p99) | Error Budget/Month |\n|---------|-------------------|--------------------|--------------------|\n| Order API | 99.95% | 500ms | 21.9 minutes |\n| Payment API | 99.99% | 200ms | 4.3 minutes |\n| Search API | 99.9% | 1000ms | 43.8 minutes |\n| User API | 99.95% | 300ms | 21.9 minutes |\n\nWhen error budget is exhausted, the team must prioritize reliability work over feature development until the budget is replenished.\n\n---\n\n## Logging (Datadog Logs)\n\n### Log Levels\n\n| Level | Usage | Sampled in Prod |\n|-------|-------|-----------------|\n| ERROR | Unexpected failures requiring investigation | 100% |\n| WARN | Degraded behavior, retryable errors | 100% |\n| INFO | Key business events, request lifecycle | 10% |\n| DEBUG | Detailed diagnostic information | 0% (enable via feature flag) |\n\n### Structured Logging Format\n\nAll services must use structured JSON logging:\n\n```json\n{\n  \"timestamp\": \"2025-10-15T14:30:00.123Z\",\n  \"level\": \"ERROR\",\n  \"service\": \"order-service\",\n  \"trace_id\": \"abc123\",\n  \"span_id\": \"def456\",\n  \"message\": \"Failed to process order\",\n  \"error\": {\n    \"type\": \"PaymentDeclined\",\n    \"message\": \"Card expired\",\n    \"stack_trace\": \"...\"\n  },\n  \"context\": {\n    \"order_id\": \"ord_789\",\n    \"user_id\": \"usr_012\",\n    \"amount_cents\": 5999\n  }\n}\n```\n\n### Rust Logging Setup\n\n```rust\nuse tracing::{info, error, instrument};\nuse tracing_subscriber::{layer::SubscriberExt, util::SubscriberInitExt};\n\n#[instrument(skip(db_pool))]\nasync fn process_order(order_id: &str, db_pool: &PgPool) -> Result<()> {\n    info!(order_id, \"Processing order\");\n    \n    match do_payment(order_id).await {\n        Ok(payment) => {\n            info!(order_id, payment_id = %payment.id, \"Payment succeeded\");\n            Ok(())\n        }\n        Err(e) => {\n            error!(order_id, error = %e, \"Payment failed\");\n            Err(e)\n        }\n    }\n}\n```\n\n---\n\n## Distributed Tracing (Datadog APM)\n\n### Trace Propagation\n\nAll services must propagate trace context via HTTP headers:\n\n```\ntraceparent: 00-abc123-def456-01\ntracestate: dd=t.dm:-1\n```\n\n### Key Traces to Monitor\n\n1. **Order lifecycle:** API \u2192 Validation \u2192 Payment \u2192 Fulfillment \u2192 Notification\n2. **Search pipeline:** Query \u2192 Parse \u2192 Execute \u2192 Rank \u2192 Return\n3. **Data sync:** Connector \u2192 Ingest \u2192 Transform \u2192 Store \u2192 Index\n\n### Trace Sampling\n\n| Environment | Sampling Rate |\n|-------------|---------------|\n| Development | 100% |\n| Staging | 100% |\n| Production | 10% base, 100% for errors, 100% for slow requests (>1s) |\n\n---\n\n## Alerting\n\n### Alert Routing\n\n```yaml\n# PagerDuty escalation policy\n- name: \"P1 Critical\"\n  rules:\n    - notify: on-call-primary\n      delay: 0 minutes\n    - notify: on-call-secondary\n      delay: 5 minutes\n    - notify: engineering-manager\n      delay: 15 minutes\n\n- name: \"P2 Warning\"\n  rules:\n    - notify: team-slack-channel\n      delay: 0 minutes\n    - notify: on-call-primary\n      delay: 30 minutes\n```\n\n### Alert Hygiene\n\n- Every alert must have a runbook link\n- Alert fatigue review: monthly audit of alert frequency\n- Goal: < 5 actionable pages per on-call shift\n- Noisy alerts are either fixed, tuned, or deleted \u2014 never ignored\n\n---\n\n## Runbook Integration\n\nEvery alert links to a runbook in our engineering handbook. Runbooks must include:\n\n1. **What this alert means** \u2014 plain English description\n2. **Impact** \u2014 what users experience\n3. **Investigation steps** \u2014 ordered diagnostic steps\n4. **Mitigation** \u2014 immediate actions to reduce impact\n5. **Resolution** \u2014 steps to fix the root cause\n6. **Escalation** \u2014 who to page if you can't resolve\n\n", "updated_at": 1771713536}, {"id": "c7d91b56-13af-44f2-ae81-f011ba792853", "title": "onboarding-guide.md", "source": "filesystem", "source_id": "onboarding-guide.md", "body": "# Engineering Onboarding Guide\n\nWelcome to Acme Engineering! This guide will get you productive in your first two weeks.\n\n---\n\n## Week 1: Setup & Orientation\n\n### Day 1: Environment Setup\n\n1. **Laptop configuration**\n   - Install Nix package manager: `curl -L https://nixos.org/nix/install | sh`\n   - Clone the dev environment: `git clone git@github.com:acme/dev-env.git`\n   - Run setup: `cd dev-env && make setup`\n   - This installs: Rust toolchain, Node.js 20, Python 3.12, Docker, kubectl\n\n2. **Access provisioning**\n   - GitHub: Request access to `acme` org in #it-helpdesk\n   - AWS: SSO login configured via Okta \u2014 use `aws sso login --profile acme-dev`\n   - Kubernetes: Kubeconfig distributed via `dev-env` setup\n   - Datadog: Request access from your team lead\n   - PagerDuty: Added to your team's rotation after Week 2\n\n3. **First PR**\n   - Add yourself to `team-directory.yaml`\n   - This verifies your Git, CI, and code review setup works end-to-end\n\n### Day 2-3: Architecture Overview\n\n- Watch the \"System Architecture\" recording (45 min) on Loom\n- Read ADR-001 through ADR-004 in the architecture-decisions doc\n- Shadow a senior engineer during their daily work\n- Tour of the Grafana dashboards and key metrics\n\n### Day 4-5: First Feature\n\n- Pick a \"good first issue\" from your team's backlog\n- Pair with your onboarding buddy\n- Ship your first feature (however small!) by end of Week 1\n\n---\n\n## Week 2: Deep Dive\n\n### Team-Specific Training\n\nEach team has a specialized onboarding track:\n\n**Platform Team:**\n- Kubernetes cluster architecture\n- CI/CD pipeline (GitHub Actions \u2192 ArgoCD)\n- Infrastructure-as-code (Terraform + Nix)\n- On-call shadowing\n\n**Product Team:**\n- Feature flag system (LaunchDarkly)\n- A/B testing framework\n- Frontend architecture (React + Next.js)\n- User analytics pipeline\n\n**Data Team:**\n- Data warehouse architecture (Snowflake)\n- ETL pipelines (dbt + Airflow)\n- Data contracts and schema evolution\n- ML model serving infrastructure\n\n---\n\n## Development Workflow\n\n### Branch Strategy\n\nWe use trunk-based development:\n\n1. Create a short-lived feature branch from `main`\n2. Keep branches under 400 lines changed\n3. Open PR with description template\n4. Require 1 approval (2 for infrastructure changes)\n5. CI must pass: tests, linting, type checking, security scan\n6. Squash merge to `main`\n7. Auto-deploy to staging, manual promotion to production\n\n### Code Review Guidelines\n\n- Review within 4 business hours\n- Focus on: correctness, readability, test coverage, security\n- Use \"nitpick:\" prefix for style suggestions\n- Block only for: bugs, security issues, missing tests, breaking changes\n\n### Testing Requirements\n\n- Unit test coverage: minimum 80% for new code\n- Integration tests for all API endpoints\n- E2E tests for critical user flows\n- Performance benchmarks for hot paths\n\n---\n\n## Communication\n\n### Meetings\n\n- **Daily standup:** 9:15 AM, 15 minutes max\n- **Sprint planning:** Monday 10 AM (bi-weekly)\n- **Retro:** Friday 3 PM (bi-weekly)\n- **Tech talks:** Thursday 2 PM (weekly, rotating presenter)\n- **All-hands:** Tuesday 11 AM (monthly)\n\n### Slack Channels\n\n- `#engineering` \u2014 General engineering discussion\n- `#incidents` \u2014 Active incident coordination\n- `#deploys` \u2014 Deployment notifications\n- `#code-review` \u2014 PR review requests\n- `#til` \u2014 Today I Learned (share interesting discoveries)\n- `#random` \u2014 Water cooler chat\n\n---\n\n## Key Contacts\n\n| Role | Person | Slack |\n|------|--------|-------|\n| VP Engineering | Dana Morrison | @dana |\n| Platform Lead | Alex Kim | @alexk |\n| Product Lead | Jordan Taylor | @jtaylor |\n| Data Lead | Priya Patel | @priya |\n| SRE Lead | Marcus Rivera | @marcus |\n| HR / People Ops | Sam Williams | @samw |\n\n", "updated_at": 1771713459}, {"id": "e99d4d1b-b29f-4be3-a36e-63ef91be7dc6", "title": "postmortem-2025-10-03.md", "source": "filesystem", "source_id": "postmortem-2025-10-03.md", "body": "# Post-Mortem: Order Processing Outage \u2014 October 3, 2025\n\n**Severity:** SEV-1  \n**Duration:** 47 minutes  \n**Incident Commander:** Marcus Rivera  \n**Author:** Sarah Chen  \n\n---\n\n## Summary\n\nOn October 3, 2025 at 14:23 UTC, the order processing pipeline stopped processing new orders for 47 minutes. During this window, approximately 12,400 orders were queued but not processed, affecting 8,200 unique customers. No orders were lost \u2014 all queued orders were processed after recovery.\n\nRevenue impact was estimated at $340,000 in delayed transactions (all recovered).\n\n---\n\n## Timeline\n\n| Time (UTC) | Event |\n|------------|-------|\n| 14:23 | Kafka consumer lag alert fires for order-processor consumer group |\n| 14:25 | On-call SRE (Marcus) acknowledges alert |\n| 14:28 | Marcus identifies that all order-processor pods are in CrashLoopBackOff |\n| 14:30 | SEV-1 declared, incident channel created |\n| 14:32 | Application logs show: `panicked at 'Failed to deserialize OrderEvent: missing field \"shipping_method\"'` |\n| 14:35 | Root cause identified: a deployment to the Order API 20 minutes earlier added a required `shipping_method` field to OrderEvent |\n| 14:38 | Two options identified: (1) rollback Order API, (2) deploy fix to order-processor |\n| 14:42 | Decision: rollback Order API as it's faster |\n| 14:45 | Order API rolled back to previous version |\n| 14:50 | Order-processor pods restart and begin draining the backlog |\n| 15:10 | All queued orders processed, consumer lag returns to zero |\n| 15:15 | SEV-1 resolved, incident channel archived |\n\n---\n\n## Root Cause Analysis\n\n### 5 Whys\n\n1. **Why did the order-processor crash?**  \n   Because it couldn't deserialize the OrderEvent \u2014 the `shipping_method` field was missing.\n\n2. **Why was the field missing?**  \n   Because the Order API was deployed with a schema change that added `shipping_method` as a required field in OrderEvent, but the order-processor hadn't been updated to handle it.\n\n3. **Why wasn't the order-processor updated first?**  \n   Because the schema change wasn't flagged as a breaking change during code review. The PR description didn't mention the Kafka event schema change.\n\n4. **Why wasn't the breaking change caught in CI?**  \n   Because we don't have automated schema compatibility checks for Kafka events. Our contract tests only cover REST APIs.\n\n5. **Why don't we have schema compatibility checks for events?**  \n   Because when we adopted Kafka, we decided to use plain JSON serialization for simplicity. We haven't implemented a schema registry yet despite it being on the roadmap for Q3.\n\n### Contributing Factors\n\n- The Order API and order-processor are owned by the same team, creating a false sense of safety\n- The Kafka event schema was only documented in code comments, not in a formal contract\n- The deploy happened right before a meeting, reducing the deployer's monitoring window\n\n---\n\n## What Went Well\n\n1. **Fast detection** \u2014 Kafka consumer lag alert fired within 2 minutes\n2. **Fast response** \u2014 On-call acknowledged and triaged within 10 minutes\n3. **Clear runbook** \u2014 The \"Kafka Consumer Lag\" runbook guided investigation\n4. **No data loss** \u2014 Kafka retention ensured all events were preserved\n5. **Clean rollback** \u2014 ArgoCD rollback was smooth and took < 5 minutes\n\n---\n\n## What Could Be Improved\n\n1. **No schema validation for Kafka events** \u2014 REST contracts are tested, but event contracts are not\n2. **PR template doesn't prompt for event schema changes** \u2014 reviewers didn't think to check\n3. **No canary for event consumers** \u2014 the crash affected 100% of consumer pods immediately\n4. **Deploy monitoring window too short** \u2014 deployer didn't observe consumer health after deploy\n\n---\n\n## Action Items\n\n| # | Action | Owner | Due | Status |\n|---|--------|-------|-----|--------|\n| 1 | Implement Confluent Schema Registry for all Kafka topics | Alex Kim | 2025-11-01 | In Progress |\n| 2 | Add \"Event Schema Changes\" checkbox to PR template | Jordan Taylor | 2025-10-10 | Done |\n| 3 | Create CI check for backward-compatible schema evolution | Sarah Chen | 2025-11-15 | Not Started |\n| 4 | Implement consumer canary deployment strategy | Marcus Rivera | 2025-12-01 | Not Started |\n| 5 | Add 15-minute deploy monitoring requirement to deployment checklist | Jordan Taylor | 2025-10-07 | Done |\n| 6 | Document all Kafka event schemas in schema registry format | Orders Team | 2025-10-31 | In Progress |\n\n---\n\n## Lessons Learned\n\n> \"Making a field required in a shared event is a breaking change, even when you own both the producer and consumer. Always assume consumers deploy on their own schedule.\"\n\nThis incident reinforced the importance of backward-compatible schema evolution. The Kafka ecosystem provides tools for this (Schema Registry with BACKWARD compatibility mode), and we've deprioritized adopting them for too long.\n\n**Rule going forward:** All Kafka event schema changes must be backward-compatible. New required fields must have defaults. Field removal requires a 2-sprint deprecation window.\n\n", "updated_at": 1771713654}, {"id": "52295064-5d1d-4d5f-b455-44198c5f3c11", "title": "rust-best-practices.md", "source": "filesystem", "source_id": "rust-best-practices.md", "body": "# Rust Best Practices at Acme\n\nThese are our team conventions for writing production Rust code.\n\n---\n\n## Project Structure\n\n```\nservice-name/\n\u251c\u2500\u2500 Cargo.toml\n\u251c\u2500\u2500 src/\n\u2502   \u251c\u2500\u2500 main.rs           # Binary entry point\n\u2502   \u251c\u2500\u2500 lib.rs            # Library root (for testing)\n\u2502   \u251c\u2500\u2500 config.rs         # Configuration parsing\n\u2502   \u251c\u2500\u2500 api/              # HTTP handlers\n\u2502   \u2502   \u251c\u2500\u2500 mod.rs\n\u2502   \u2502   \u251c\u2500\u2500 routes.rs\n\u2502   \u2502   \u2514\u2500\u2500 handlers.rs\n\u2502   \u251c\u2500\u2500 domain/           # Business logic (no I/O)\n\u2502   \u2502   \u251c\u2500\u2500 mod.rs\n\u2502   \u2502   \u2514\u2500\u2500 models.rs\n\u2502   \u251c\u2500\u2500 infra/            # External integrations\n\u2502   \u2502   \u251c\u2500\u2500 mod.rs\n\u2502   \u2502   \u251c\u2500\u2500 database.rs\n\u2502   \u2502   \u2514\u2500\u2500 kafka.rs\n\u2502   \u2514\u2500\u2500 telemetry.rs      # Observability setup\n\u251c\u2500\u2500 tests/\n\u2502   \u2514\u2500\u2500 integration/\n\u2502       \u2514\u2500\u2500 api_test.rs\n\u2514\u2500\u2500 migrations/\n    \u2514\u2500\u2500 001_initial.sql\n```\n\n---\n\n## Error Handling\n\n### Use `thiserror` for Library Errors\n\n```rust\nuse thiserror::Error;\n\n#[derive(Error, Debug)]\npub enum OrderError {\n    #[error(\"Order not found: {0}\")]\n    NotFound(String),\n    \n    #[error(\"Payment declined: {reason}\")]\n    PaymentDeclined { reason: String },\n    \n    #[error(\"Insufficient inventory for product {product_id}\")]\n    InsufficientInventory { product_id: String },\n    \n    #[error(transparent)]\n    Database(#[from] sqlx::Error),\n}\n```\n\n### Use `anyhow` for Application Code\n\n```rust\nuse anyhow::{Context, Result};\n\nasync fn process_order(order_id: &str) -> Result<()> {\n    let order = db::get_order(order_id)\n        .await\n        .context(\"Failed to fetch order from database\")?;\n    \n    let payment = payment::charge(&order)\n        .await\n        .context(\"Payment processing failed\")?;\n    \n    Ok(())\n}\n```\n\n### Never Panic in Production\n\n```rust\n// \u274c Bad: panics on None\nlet user = users.get(id).unwrap();\n\n// \u2705 Good: returns error\nlet user = users.get(id)\n    .ok_or_else(|| OrderError::NotFound(id.to_string()))?;\n\n// \u274c Bad: panics on parse failure  \nlet port: u16 = env::var(\"PORT\").unwrap().parse().unwrap();\n\n// \u2705 Good: provides context\nlet port: u16 = env::var(\"PORT\")\n    .context(\"PORT environment variable not set\")?\n    .parse()\n    .context(\"PORT must be a valid u16\")?;\n```\n\n---\n\n## Async Patterns\n\n### Use Tokio as the Runtime\n\n```rust\n#[tokio::main]\nasync fn main() -> Result<()> {\n    let config = Config::from_env()?;\n    let pool = PgPoolOptions::new()\n        .max_connections(config.db.max_connections)\n        .connect(&config.db.url)\n        .await?;\n    \n    let app = Router::new()\n        .route(\"/health\", get(health))\n        .route(\"/v1/orders\", post(create_order))\n        .with_state(AppState { pool });\n    \n    let listener = TcpListener::bind(&config.bind).await?;\n    axum::serve(listener, app).await?;\n    \n    Ok(())\n}\n```\n\n### Structured Concurrency\n\n```rust\n// \u2705 Good: bounded concurrency with buffer_unordered\nuse futures::stream::{self, StreamExt};\n\nlet results: Vec<Result<Response>> = stream::iter(urls)\n    .map(|url| async move { reqwest::get(&url).await })\n    .buffer_unordered(10)  // Max 10 concurrent requests\n    .collect()\n    .await;\n\n// \u274c Bad: unbounded spawn\nfor url in urls {\n    tokio::spawn(async move { reqwest::get(&url).await });\n}\n```\n\n### Graceful Shutdown\n\n```rust\nlet (shutdown_tx, shutdown_rx) = tokio::sync::oneshot::channel();\n\ntokio::spawn(async move {\n    tokio::signal::ctrl_c().await.unwrap();\n    shutdown_tx.send(()).unwrap();\n});\n\naxum::serve(listener, app)\n    .with_graceful_shutdown(async { shutdown_rx.await.ok(); })\n    .await?;\n```\n\n---\n\n## Testing\n\n### Unit Tests (Pure Logic)\n\n```rust\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    #[test]\n    fn test_calculate_shipping() {\n        let order = Order {\n            items: vec![item(10_00, 2), item(25_00, 1)],\n            destination: Address::domestic(),\n        };\n        \n        assert_eq!(calculate_shipping(&order), 5_99);\n    }\n    \n    #[test]\n    fn test_free_shipping_over_threshold() {\n        let order = Order {\n            items: vec![item(50_00, 1)],\n            destination: Address::domestic(),\n        };\n        \n        assert_eq!(calculate_shipping(&order), 0);\n    }\n}\n```\n\n### Integration Tests (With Database)\n\n```rust\n#[sqlx::test(migrations = \"migrations\")]\nasync fn test_create_and_fetch_order(pool: PgPool) {\n    let order = CreateOrder {\n        user_id: \"usr_123\".into(),\n        items: vec![OrderItem { product_id: \"prod_456\".into(), quantity: 2 }],\n    };\n    \n    let created = OrderRepo::create(&pool, &order).await.unwrap();\n    assert!(created.id.starts_with(\"ord_\"));\n    \n    let fetched = OrderRepo::get(&pool, &created.id).await.unwrap();\n    assert_eq!(fetched.user_id, \"usr_123\");\n    assert_eq!(fetched.items.len(), 1);\n}\n```\n\n---\n\n## Performance\n\n### Avoid Unnecessary Allocations\n\n```rust\n// \u274c Bad: allocates a new String\nfn greet(name: &str) -> String {\n    format!(\"Hello, {}!\", name)\n}\n\n// \u2705 Good: accepts any string-like type\nfn process(input: impl AsRef<str>) {\n    let s = input.as_ref();\n    // work with &str\n}\n\n// \u2705 Good: use Cow for conditional ownership\nuse std::borrow::Cow;\n\nfn normalize(input: &str) -> Cow<'_, str> {\n    if input.contains(' ') {\n        Cow::Owned(input.replace(' ', \"_\"))\n    } else {\n        Cow::Borrowed(input)\n    }\n}\n```\n\n### Use `#[inline]` Judiciously\n\nOnly inline small, hot functions. Let the compiler decide for everything else.\n\n### Profile Before Optimizing\n\n```bash\n# CPU profiling\ncargo flamegraph --bin order-service\n\n# Memory profiling  \nDHAT_OUT_FILE=dhat.json cargo run --features dhat-heap\n```\n\n---\n\n## Dependencies\n\n### Approved Crates\n\n| Category | Crate | Version Policy |\n|----------|-------|----------------|\n| HTTP server | axum | Latest stable |\n| HTTP client | reqwest | Latest stable |\n| Database | sqlx | Latest stable |\n| Serialization | serde + serde_json | Latest stable |\n| Async runtime | tokio | Latest stable |\n| Error handling | anyhow + thiserror | Latest stable |\n| Logging | tracing | Latest stable |\n| CLI | clap | Latest stable |\n| Time | chrono | Latest stable |\n| UUID | uuid | Latest stable |\n\n### Adding New Dependencies\n\n1. Check if an existing approved crate covers the use case\n2. Evaluate: maintenance activity, security history, compile time impact\n3. Get approval from team lead for new dependencies\n4. Run `cargo audit` after adding\n\n", "updated_at": 1771713686}, {"id": "c9819917-3409-4c5b-8163-7d51a4925c02", "title": "security-practices.md", "source": "filesystem", "source_id": "security-practices.md", "body": "# Security Practices\n\nSecurity is everyone's responsibility. These practices are mandatory for all engineers.\n\n---\n\n## Authentication & Authorization\n\n### Identity Provider\n\nAll human access goes through Okta SSO with mandatory MFA:\n\n- **Primary factor:** Okta Verify push notification\n- **Backup factors:** TOTP codes, hardware security keys (YubiKey)\n- **Session duration:** 12 hours for standard access, 1 hour for admin access\n- **Conditional access:** VPN required for production access from non-corporate networks\n\n### Service Identity\n\nServices authenticate using short-lived certificates managed by the Istio service mesh:\n\n1. Each pod gets a unique SPIFFE identity: `spiffe://acme.com/ns/production/sa/order-service`\n2. Certificates auto-rotate every 24 hours\n3. mTLS enforced for all inter-service communication\n4. Authorization policies define which services can communicate\n\n```yaml\napiVersion: security.istio.io/v1\nkind: AuthorizationPolicy\nmetadata:\n  name: order-service-policy\nspec:\n  selector:\n    matchLabels:\n      app: order-service\n  rules:\n    - from:\n        - source:\n            principals: [\"cluster.local/ns/production/sa/api-gateway\"]\n      to:\n        - operation:\n            methods: [\"GET\", \"POST\"]\n            paths: [\"/v1/orders*\"]\n```\n\n---\n\n## Secrets Management\n\n### HashiCorp Vault\n\nAll secrets are stored in Vault and injected at runtime:\n\n```bash\n# Application reads secrets from Vault agent sidecar\nvault kv get secret/order-service/database\n```\n\n### Secret Rotation\n\n| Secret Type | Rotation Period | Automated |\n|-------------|-----------------|-----------|\n| Database passwords | 30 days | Yes |\n| API keys | 90 days | Yes |\n| TLS certificates | 24 hours | Yes (cert-manager) |\n| Encryption keys | 365 days | Yes (AWS KMS) |\n\n### Never Do This\n\n- \u274c Commit secrets to Git (even in private repos)\n- \u274c Store secrets in environment variables in CI\n- \u274c Share secrets via Slack or email\n- \u274c Use long-lived API keys for service accounts\n- \u274c Hardcode credentials in application code\n\n---\n\n## Data Protection\n\n### Encryption\n\n| Layer | Method | Key Management |\n|-------|--------|----------------|\n| At rest | AES-256 | AWS KMS |\n| In transit | TLS 1.3 | cert-manager |\n| Application-level | AES-256-GCM | Vault Transit |\n\n### PII Handling\n\nPersonal Identifiable Information requires special handling:\n\n1. **Classification:** All data fields must be classified (PII, Sensitive, Internal, Public)\n2. **Minimization:** Only collect PII that is strictly necessary\n3. **Tokenization:** PII is tokenized before storage; original values in a separate token vault\n4. **Retention:** PII deleted after retention period (default: 2 years, configurable per regulation)\n5. **Access logging:** All PII access is logged and auditable\n\n### GDPR Compliance\n\n- Right to erasure: automated deletion pipeline triggered via admin API\n- Data portability: export endpoint returns all user data in JSON format\n- Consent tracking: granular consent stored per data processing purpose\n- Cross-border transfers: EU data stays in eu-west-1 region\n\n---\n\n## Vulnerability Management\n\n### Dependency Scanning\n\n```bash\n# Run in CI on every PR\ncargo audit                    # Rust advisory database\nsnyk test --all-projects      # Transitive dependency scan\ntrivy image acme/order:latest  # Container vulnerability scan\n```\n\n### Severity Response Times\n\n| Severity | Response | Fix Deadline |\n|----------|----------|-------------|\n| Critical (CVSS 9.0+) | Immediate page | 24 hours |\n| High (CVSS 7.0-8.9) | Same-day triage | 1 week |\n| Medium (CVSS 4.0-6.9) | Sprint planning | 1 sprint |\n| Low (CVSS 0.1-3.9) | Backlog | Next quarter |\n\n### Penetration Testing\n\n- Annual third-party pentest of all external-facing services\n- Quarterly internal red team exercises\n- Bug bounty program (HackerOne) with payouts up to $10,000\n\n---\n\n## Incident Security Response\n\nIf you suspect a security incident:\n\n1. **Do not try to fix it yourself** \u2014 contain and escalate\n2. Notify `#security-incidents` Slack channel immediately\n3. Page the security on-call: `pd trigger security`\n4. Preserve evidence \u2014 do not delete logs or modify systems\n5. Document everything with timestamps\n\n### Security Incident Classification\n\n| Type | Example | Escalation |\n|------|---------|------------|\n| Data breach | PII exposed externally | CEO, Legal, CISO |\n| Unauthorized access | Suspicious login patterns | Security team |\n| Malware | Compromised dependency | Security + Platform |\n| Social engineering | Phishing targeting engineers | Security + HR |\n\n", "updated_at": 1771713592}, {"id": "57e4e4f6-6c95-4d2e-a24c-800291a50530", "title": "team-topology.md", "source": "filesystem", "source_id": "team-topology.md", "body": "# Team Topology & Ownership\n\nAcme Engineering is organized around Team Topologies principles with stream-aligned teams, platform teams, and enabling teams.\n\n---\n\n## Stream-Aligned Teams\n\nThese teams own end-to-end business capabilities and deliver value directly to users.\n\n### Orders Team\n\n**Mission:** Own the order lifecycle from cart to delivery  \n**Lead:** Jordan Taylor  \n**Size:** 6 engineers  \n**Stack:** Rust (backend), React (frontend), PostgreSQL, Kafka  \n\n**Owns:**\n- Order creation and management API\n- Payment processing integration (Stripe)\n- Order fulfillment workflow\n- Order status notifications\n- Returns and refunds\n\n**Key Metrics:**\n- Order success rate: > 99.5%\n- Checkout latency p99: < 2 seconds\n- Refund processing time: < 24 hours\n\n---\n\n### Search & Discovery Team\n\n**Mission:** Help users find what they're looking for  \n**Lead:** Wei Zhang  \n**Size:** 5 engineers  \n**Stack:** Rust (ranking engine), Elasticsearch, Python (ML models)  \n\n**Owns:**\n- Search API and ranking algorithm\n- Product recommendations\n- Browse and filter experience\n- Search analytics and A/B testing\n- Autocomplete and spell correction\n\n**Key Metrics:**\n- Search latency p99: < 500ms\n- Click-through rate on first page: > 35%\n- Zero-results rate: < 5%\n\n---\n\n### User Experience Team\n\n**Mission:** Authentication, profiles, and personalization  \n**Lead:** Rachel Adams  \n**Size:** 4 engineers  \n**Stack:** TypeScript (Next.js), PostgreSQL, Redis  \n\n**Owns:**\n- User registration and authentication\n- Profile management\n- Notification preferences\n- Wishlist and saved items\n- Personalization signals\n\n---\n\n## Platform Team\n\n### Infrastructure & Platform\n\n**Mission:** Make it easy and safe for stream-aligned teams to deliver  \n**Lead:** Alex Kim  \n**Size:** 7 engineers  \n**Stack:** Kubernetes, Terraform, Nix, Go, Rust  \n\n**Owns:**\n- Kubernetes cluster management\n- CI/CD pipeline (GitHub Actions \u2192 ArgoCD)\n- Service mesh (Istio)\n- Container registry and build systems\n- Developer experience tooling\n- Cost optimization\n\n**SLAs:**\n- CI build time: < 10 minutes for 95th percentile\n- Deployment pipeline: < 30 minutes from merge to production\n- Platform availability: > 99.99%\n\n---\n\n### Data Platform\n\n**Mission:** Enable data-driven decisions across all teams  \n**Lead:** Priya Patel  \n**Size:** 5 engineers  \n**Stack:** Snowflake, dbt, Airflow, Kafka, Flink, Python  \n\n**Owns:**\n- Data warehouse (Snowflake)\n- ETL pipelines and data quality\n- Feature store (Feast)\n- ML training and serving infrastructure\n- Data governance and access control\n\n---\n\n## Enabling Teams\n\n### SRE Team\n\n**Mission:** Reliability across all production services  \n**Lead:** Marcus Rivera  \n**Size:** 4 engineers  \n\n**Provides:**\n- On-call support and incident management\n- SLO/SLI definitions and monitoring\n- Capacity planning\n- Chaos engineering exercises\n- Reliability consulting for new services\n\n**Engagement model:** SRE reviews all new service launches and provides reliability guidance. Teams with services below SLO get embedded SRE support.\n\n---\n\n### Security Team\n\n**Mission:** Protect our systems and customer data  \n**Lead:** Nadia Hassan  \n**Size:** 3 engineers  \n\n**Provides:**\n- Security architecture reviews\n- Penetration testing\n- Vulnerability management\n- Compliance (SOC 2, GDPR)\n- Security training and awareness\n\n---\n\n## Service Ownership Matrix\n\n| Service | Owning Team | On-Call | Dependencies |\n|---------|-------------|---------|--------------|\n| Order API | Orders | Orders | Payment, Inventory |\n| Payment Service | Orders | Orders | Stripe API |\n| Search API | Search | Search | Elasticsearch, ML |\n| Ranking Engine | Search | Search | Feature Store |\n| User Service | User Experience | UX | PostgreSQL, Redis |\n| Auth Service | User Experience | UX | Okta, Vault |\n| API Gateway | Platform | Platform | Istio |\n| Data Pipeline | Data Platform | Data | Snowflake, Kafka |\n\n---\n\n## Cross-Team Communication\n\n### Dependency Requests\n\nWhen your team needs something from another team:\n\n1. Open a Jira ticket in the provider team's backlog\n2. Label it `cross-team` with priority and deadline\n3. Attend their sprint planning if it's urgent\n4. For API changes, propose via RFC in #engineering-rfcs\n\n### RFC Process\n\nMajor cross-team changes require an RFC:\n\n1. Author writes RFC using template in `docs/rfcs/`\n2. Post in #engineering-rfcs for async review (1 week)\n3. Schedule review meeting if needed\n4. Approvals required from affected team leads\n5. Decision documented and shared\n\n---\n\n## Team Health\n\nWe measure team health quarterly using these dimensions:\n\n1. **Delivery speed** \u2014 How quickly can the team ship value?\n2. **Quality** \u2014 Error rates, incident frequency, tech debt trend\n3. **Autonomy** \u2014 Can the team deliver without cross-team blockers?\n4. **Learning** \u2014 Is the team growing skills and improving processes?\n5. **Satisfaction** \u2014 Do team members enjoy their work?\n\nResults are discussed in retros and shared with engineering leadership.\n\n", "updated_at": 1771713621}], "chunks": [{"id": "2c5f988b-69ec-4b04-84a0-495001864a1d", "document_id": "0629f23f-83b3-4e34-ae50-cd5fb29ddaa9", "chunk_index": 0, "text": "# API Design Standards\n\nAll public and internal APIs at Acme must follow these standards.\n\n---\n\n## RESTful API Conventions\n\n### URL Structure\n\n```\n/{version}/{resource}/{id}/{sub-resource}\n```\n\nExamples:\n- `GET /v1/orders` \u2014 list orders\n- `GET /v1/orders/123` \u2014 get order by ID\n- `POST /v1/orders` \u2014 create order\n- `PUT /v1/orders/123` \u2014 update order\n- `DELETE /v1/orders/123` \u2014 delete order\n- `GET /v1/orders/123/items` \u2014 list items for order\n\n### HTTP Methods\n\n| Method | Usage | Idempotent |\n|--------|-------|------------|\n| GET | Read resource(s) | Yes |\n| POST | Create resource | No |\n| PUT | Full update | Yes |\n| PATCH | Partial update | Yes |\n| DELETE | Remove resource | Yes |\n\n### Response Codes\n\n| Code | Usage |\n|------|-------|\n| 200 | Success |\n| 201 | Created (POST) |\n| 204 | No Content (DELETE) |\n| 400 | Bad Request \u2014 validation failed |\n| 401 | Unauthorized \u2014 missing/invalid token |\n| 403 | Forbidden \u2014 insufficient permissions |\n| 404 | Not Found |\n| 409 | Conflict \u2014 duplicate resource |\n| 422 | Unprocessable Entity \u2014 business rule violation |\n| 429 | Too Many Requests \u2014 rate limited |\n| 500 | Internal Server Error |\n| 503 | Service Unavailable |\n\n---\n\n## Pagination\n\nAll list endpoints must support cursor-based pagination:\n\n```json\n{\n  \"data\": [...],\n  \"pagination\": {\n    \"cursor\": \"eyJpZCI6MTIzfQ==\",\n    \"has_more\": true,\n    \"total_count\": 1547\n  }\n}\n```\n\nQuery parameters:\n- `cursor` \u2014 opaque cursor from previous response\n- `limit` \u2014 items per page (default: 20, max: 100)\n- `sort` \u2014 field to sort by (default: `created_at`)\n- `order` \u2014 `asc` or `desc` (default: `desc`)\n\n---\n\n## Error Format\n\nAll errors must follow this structure:"}, {"id": "de72414f-ef24-43cb-8b8c-fc780cce8422", "document_id": "0629f23f-83b3-4e34-ae50-cd5fb29ddaa9", "chunk_index": 1, "text": "```json\n{\n  \"error\": {\n    \"code\": \"validation_failed\",\n    \"message\": \"Human-readable error message\",\n    \"details\": [\n      {\n        \"field\": \"email\",\n        \"message\": \"must be a valid email address\",\n        \"code\": \"invalid_format\"\n      }\n    ],\n    \"request_id\": \"req_abc123\",\n    \"timestamp\": \"2025-10-15T14:30:00Z\"\n  }\n}\n```\n\n### Standard Error Codes\n\n- `validation_failed` \u2014 request body/params invalid\n- `not_found` \u2014 resource does not exist\n- `unauthorized` \u2014 authentication required\n- `forbidden` \u2014 insufficient permissions\n- `conflict` \u2014 resource already exists\n- `rate_limited` \u2014 too many requests\n- `internal_error` \u2014 unexpected server error\n\n---\n\n## Authentication\n\nAll APIs use JWT Bearer tokens issued by our OAuth2 server:\n\n```\nAuthorization: Bearer eyJhbGciOiJSUzI1NiIs...\n```\n\n### Token Scopes\n\n| Scope | Access |\n|-------|--------|\n| `read:orders` | Read order data |\n| `write:orders` | Create/update orders |\n| `admin:orders` | Delete orders, manage rules |\n| `read:users` | Read user profiles |\n| `admin:users` | Manage user accounts |\n\n### Service-to-Service Authentication\n\nInternal services use mTLS with short-lived certificates rotated every 24 hours. The service mesh (Istio) handles certificate management transparently.\n\n---\n\n## Rate Limiting\n\n| Tier | Limit | Window |\n|------|-------|--------|\n| Standard | 100 req/min | Per API key |\n| Premium | 1000 req/min | Per API key |\n| Internal | 10000 req/min | Per service identity |\n\nRate limit headers in every response:\n```\nX-RateLimit-Limit: 100\nX-RateLimit-Remaining: 87\nX-RateLimit-Reset: 1697376000\n```\n\n---\n\n## Versioning\n\n- URL-based versioning: `/v1/`, `/v2/`\n- Major version increments only for breaking changes\n- Deprecation: minimum 6-month sunset period with `Sunset` header\n- Breaking changes require ADR and cross-team review\n\n---\n\n## OpenAPI Specification\n\nEvery API must have an OpenAPI 3.1 spec committed to the repo:"}, {"id": "7f3dc628-8d7b-477f-8534-246fda57af85", "document_id": "0629f23f-83b3-4e34-ae50-cd5fb29ddaa9", "chunk_index": 2, "text": "```\nservice-name/\n\u251c\u2500\u2500 api/\n\u2502   \u2514\u2500\u2500 openapi.yaml\n\u251c\u2500\u2500 src/\n\u2514\u2500\u2500 tests/\n    \u2514\u2500\u2500 api-contract/\n        \u2514\u2500\u2500 contract_test.rs\n```\n\nContract tests validate that the implementation matches the spec. CI blocks merge if specs diverge."}, {"id": "c8e6cb6d-f35a-4f8e-bb75-42aef3b0b4de", "document_id": "2f0a6ba0-7a83-49db-bb82-49e411732669", "chunk_index": 0, "text": "# Incident Response Playbook\n\n## Severity Levels\n\n### SEV-1: Critical Production Outage\n- **Definition:** Complete loss of service for all users, data loss risk, or security breach\n- **Response time:** 15 minutes\n- **Escalation:** VP Engineering + On-call SRE + affected team leads\n- **Communication:** Status page updated every 15 minutes, exec Slack channel, customer success notified\n\n### SEV-2: Major Degradation\n- **Definition:** Significant feature unavailable, performance degraded >50%, data integrity concerns\n- **Response time:** 30 minutes\n- **Escalation:** On-call SRE + team lead\n- **Communication:** Status page updated every 30 minutes, engineering Slack channel\n\n### SEV-3: Minor Issue\n- **Definition:** Non-critical feature affected, workaround available, isolated impact\n- **Response time:** 4 hours (business hours)\n- **Escalation:** Team on-call\n- **Communication:** Team Slack channel, tracked in Jira\n\n---\n\n## On-Call Rotation\n\nTeams rotate weekly, Monday to Monday. The on-call engineer must:\n\n1. Acknowledge pages within 5 minutes\n2. Have laptop and reliable internet access at all times\n3. Be within 15 minutes of a workstation\n4. Escalate if unable to triage within 30 minutes\n5. Hand off to next on-call with a written summary\n\n### Compensation\n- $500/week flat on-call stipend\n- Additional $200 for each SEV-1 incident handled\n- Comp time: 4 hours off for each overnight page\n\n---\n\n## Incident Timeline Template\n\n```\nINCIDENT: [Short description]\nSEVERITY: SEV-[1/2/3]\nSTARTED: [Timestamp UTC]\nDETECTED: [Timestamp UTC] \u2014 [How detected: alert/customer report/monitoring]\nACKNOWLEDGED: [Timestamp UTC] \u2014 [Who]\nMITIGATED: [Timestamp UTC] \u2014 [What action]\nRESOLVED: [Timestamp UTC]\nDURATION: [Total time]\n\nIMPACT:\n- Users affected: [number/percentage]\n- Revenue impact: [estimate]\n- Data loss: [yes/no, details]\n\nROOT CAUSE:\n[Description]\n\nTIMELINE:\n[HH:MM] \u2014 Event description\n[HH:MM] \u2014 Event description"}, {"id": "2869d6de-dd9e-468a-96dd-129eb73d5cdd", "document_id": "2f0a6ba0-7a83-49db-bb82-49e411732669", "chunk_index": 1, "text": "ACTION ITEMS:\n- [ ] [Description] \u2014 Owner: [name] \u2014 Due: [date]\n```\n\n---\n\n## Post-Incident Review Process\n\nEvery SEV-1 and SEV-2 incident requires a blameless post-mortem within 48 hours.\n\n### Rules\n1. **Blameless culture** \u2014 Focus on systems, not individuals\n2. **5 Whys** \u2014 Dig into root causes, not symptoms\n3. **Action items must be tracked** \u2014 Each item gets a Jira ticket with an owner and due date\n4. **Share learnings** \u2014 Post-mortem document shared in #engineering and presented at weekly all-hands\n\n### Post-Mortem Template\n- Incident summary\n- Timeline of events\n- Root cause analysis (5 Whys)\n- What went well\n- What could be improved\n- Action items with owners and deadlines\n\n---\n\n## Common Runbooks\n\n### Database Connection Pool Exhaustion\n1. Check current connections: `SELECT count(*) FROM pg_stat_activity;`\n2. Identify long-running queries: `SELECT * FROM pg_stat_activity WHERE state = 'active' ORDER BY query_start;`\n3. Kill idle connections older than 10 minutes\n4. Verify PgBouncer pool settings\n5. Check for connection leaks in application logs\n\n### Kafka Consumer Lag\n1. Check consumer group lag: `kafka-consumer-groups --describe --group <group>`\n2. Verify consumer health in Grafana dashboard\n3. Check for poison messages in dead letter queue\n4. Scale consumer instances if processing throughput is the bottleneck\n5. Verify schema compatibility if deserialization errors are present\n\n### Memory Pressure on Kubernetes Pods\n1. Check pod memory usage: `kubectl top pods -n <namespace>`\n2. Review recent deployments for memory regression\n3. Check for memory leaks using heap profiler\n4. Adjust resource limits if growth is expected\n5. Consider horizontal pod autoscaler (HPA) configuration"}, {"id": "4e4fbfe9-89ad-4e86-943a-1e2f53b40d02", "document_id": "3d3a08e5-1553-4cdc-97a3-e212cbbcc79b", "chunk_index": 0, "text": "# Deployment Pipeline\n\nOur CI/CD pipeline ensures safe, automated deployments from commit to production.\n\n---\n\n## Pipeline Stages\n\n```\nCommit \u2192 Build \u2192 Test \u2192 Security Scan \u2192 Deploy Staging \u2192 Canary \u2192 Production\n```\n\n### 1. Build (GitHub Actions)\n\nTriggered on every push to `main` and on PR creation.\n\n```yaml\njobs:\n  build:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n      - name: Build\n        run: cargo build --release\n      - name: Lint\n        run: cargo clippy -- -D warnings\n      - name: Format check\n        run: cargo fmt --check\n      - name: Upload artifact\n        uses: actions/upload-artifact@v4\n```\n\nBuild artifacts are Docker images pushed to our private ECR registry with the Git SHA as the tag.\n\n### 2. Test Suite\n\nThree test tiers run in parallel:\n\n| Tier | Duration | What |\n|------|----------|------|\n| Unit | ~2 min | Pure function tests, mocked dependencies |\n| Integration | ~8 min | Database, message queue, external service tests |\n| E2E | ~15 min | Full user flow through the UI |\n\n**Flaky test policy:** If a test fails intermittently more than 3 times in 30 days, it's quarantined and the owning team has 1 sprint to fix or delete it.\n\n### 3. Security Scanning\n\n- **SAST:** Semgrep rules for common vulnerabilities\n- **Dependency audit:** `cargo audit` + Snyk for transitive dependencies\n- **Container scan:** Trivy scans Docker images for known CVEs\n- **Secret detection:** TruffleHog prevents accidental credential commits\n\nCritical findings block deployment. High findings must be addressed within 1 sprint.\n\n### 4. Staging Deployment\n\nArgoCD watches the staging branch and auto-syncs:\n\n```yaml\napiVersion: argoproj.io/v1alpha1\nkind: Application\nmetadata:\n  name: order-service-staging\nspec:\n  destination:\n    namespace: staging\n    server: https://kubernetes.default.svc\n  source:\n    repoURL: https://github.com/acme/deployments\n    path: services/order-service/staging\n```"}, {"id": "335da11b-62bc-4295-88b6-a74cb0567874", "document_id": "3d3a08e5-1553-4cdc-97a3-e212cbbcc79b", "chunk_index": 1, "text": "Staging mirrors production topology at 1/10th scale. Synthetic traffic generators simulate real user patterns.\n\n### 5. Canary Deployment\n\nProduction deployments use a canary strategy:\n\n1. Deploy new version to 5% of pods\n2. Monitor error rate, latency p99, and business metrics for 15 minutes\n3. If metrics are healthy, ramp to 25% \u2192 50% \u2192 100% over 1 hour\n4. Automatic rollback if error rate increases >0.5% or p99 latency increases >20%\n\n```yaml\napiVersion: flagger.app/v1beta1\nkind: Canary\nspec:\n  analysis:\n    interval: 1m\n    threshold: 5\n    maxWeight: 50\n    stepWeight: 10\n  metrics:\n    - name: request-success-rate\n      thresholdRange:\n        min: 99.5\n    - name: request-duration\n      thresholdRange:\n        max: 500\n```\n\n---\n\n## Rollback Procedures\n\n### Automatic Rollback\n\nCanary deployments auto-rollback when metrics breach thresholds. No human intervention needed.\n\n### Manual Rollback\n\nFor post-deploy issues discovered after canary graduation:\n\n```bash\n# Rollback to previous version\nkubectl rollout undo deployment/order-service -n production\n\n# Or deploy a specific version\nargocd app set order-service --revision <git-sha>\nargocd app sync order-service\n```\n\n### Database Rollback\n\nDatabase migrations are forward-only. To \"rollback\" a migration:\n\n1. Create a new migration that reverts the schema change\n2. Ensure the application code handles both old and new schema\n3. Deploy the reverting migration\n4. Deploy the application rollback\n\n**Rule:** Never use `DROP COLUMN` or `ALTER COLUMN` in a migration without a 2-phase deploy strategy.\n\n---\n\n## Environment Promotion\n\n| Environment | Purpose | Data | Scale |\n|-------------|---------|------|-------|\n| Dev | Local development | Synthetic | Single node |\n| Staging | Integration testing | Anonymized production | 1/10th prod |\n| Production | Live traffic | Real | Full scale |\n\nFeature flags (LaunchDarkly) control feature availability independently of deployments. This decouples deploy from release.\n\n---"}, {"id": "248ef0c7-e033-4ac3-8337-71fdf14ad429", "document_id": "3d3a08e5-1553-4cdc-97a3-e212cbbcc79b", "chunk_index": 2, "text": "## Deployment Schedule\n\n- **Staging:** Continuous (every merge to main)\n- **Production:** Business hours only (9 AM - 4 PM ET, Mon-Thu)\n- **Freeze periods:** 2 weeks before major product launches, last week of each quarter\n- **Emergency deploys:** Anytime with SEV-1 incident commander approval"}, {"id": "cc1857b6-51b1-407c-805a-e0e97a38f924", "document_id": "52295064-5d1d-4d5f-b455-44198c5f3c11", "chunk_index": 0, "text": "# Rust Best Practices at Acme\n\nThese are our team conventions for writing production Rust code.\n\n---\n\n## Project Structure\n\n```\nservice-name/\n\u251c\u2500\u2500 Cargo.toml\n\u251c\u2500\u2500 src/\n\u2502   \u251c\u2500\u2500 main.rs           # Binary entry point\n\u2502   \u251c\u2500\u2500 lib.rs            # Library root (for testing)\n\u2502   \u251c\u2500\u2500 config.rs         # Configuration parsing\n\u2502   \u251c\u2500\u2500 api/              # HTTP handlers\n\u2502   \u2502   \u251c\u2500\u2500 mod.rs\n\u2502   \u2502   \u251c\u2500\u2500 routes.rs\n\u2502   \u2502   \u2514\u2500\u2500 handlers.rs\n\u2502   \u251c\u2500\u2500 domain/           # Business logic (no I/O)\n\u2502   \u2502   \u251c\u2500\u2500 mod.rs\n\u2502   \u2502   \u2514\u2500\u2500 models.rs\n\u2502   \u251c\u2500\u2500 infra/            # External integrations\n\u2502   \u2502   \u251c\u2500\u2500 mod.rs\n\u2502   \u2502   \u251c\u2500\u2500 database.rs\n\u2502   \u2502   \u2514\u2500\u2500 kafka.rs\n\u2502   \u2514\u2500\u2500 telemetry.rs      # Observability setup\n\u251c\u2500\u2500 tests/\n\u2502   \u2514\u2500\u2500 integration/\n\u2502       \u2514\u2500\u2500 api_test.rs\n\u2514\u2500\u2500 migrations/\n    \u2514\u2500\u2500 001_initial.sql\n```\n\n---\n\n## Error Handling\n\n### Use `thiserror` for Library Errors\n\n```rust\nuse thiserror::Error;\n\n#[derive(Error, Debug)]\npub enum OrderError {\n    #[error(\"Order not found: {0}\")]\n    NotFound(String),\n    \n    #[error(\"Payment declined: {reason}\")]\n    PaymentDeclined { reason: String },\n    \n    #[error(\"Insufficient inventory for product {product_id}\")]\n    InsufficientInventory { product_id: String },\n    \n    #[error(transparent)]\n    Database(#[from] sqlx::Error),\n}\n```\n\n### Use `anyhow` for Application Code\n\n```rust\nuse anyhow::{Context, Result};\n\nasync fn process_order(order_id: &str) -> Result<()> {\n    let order = db::get_order(order_id)\n        .await\n        .context(\"Failed to fetch order from database\")?;\n    \n    let payment = payment::charge(&order)\n        .await\n        .context(\"Payment processing failed\")?;\n    \n    Ok(())\n}\n```\n\n### Never Panic in Production\n\n```rust\n// \u274c Bad: panics on None\nlet user = users.get(id).unwrap();"}, {"id": "b348f681-7beb-4ae6-a8ec-762a5bd44d57", "document_id": "52295064-5d1d-4d5f-b455-44198c5f3c11", "chunk_index": 1, "text": "// \u2705 Good: returns error\nlet user = users.get(id)\n    .ok_or_else(|| OrderError::NotFound(id.to_string()))?;\n\n// \u274c Bad: panics on parse failure  \nlet port: u16 = env::var(\"PORT\").unwrap().parse().unwrap();\n\n// \u2705 Good: provides context\nlet port: u16 = env::var(\"PORT\")\n    .context(\"PORT environment variable not set\")?\n    .parse()\n    .context(\"PORT must be a valid u16\")?;\n```\n\n---\n\n## Async Patterns\n\n### Use Tokio as the Runtime\n\n```rust\n#[tokio::main]\nasync fn main() -> Result<()> {\n    let config = Config::from_env()?;\n    let pool = PgPoolOptions::new()\n        .max_connections(config.db.max_connections)\n        .connect(&config.db.url)\n        .await?;\n    \n    let app = Router::new()\n        .route(\"/health\", get(health))\n        .route(\"/v1/orders\", post(create_order))\n        .with_state(AppState { pool });\n    \n    let listener = TcpListener::bind(&config.bind).await?;\n    axum::serve(listener, app).await?;\n    \n    Ok(())\n}\n```\n\n### Structured Concurrency\n\n```rust\n// \u2705 Good: bounded concurrency with buffer_unordered\nuse futures::stream::{self, StreamExt};\n\nlet results: Vec<Result<Response>> = stream::iter(urls)\n    .map(|url| async move { reqwest::get(&url).await })\n    .buffer_unordered(10)  // Max 10 concurrent requests\n    .collect()\n    .await;\n\n// \u274c Bad: unbounded spawn\nfor url in urls {\n    tokio::spawn(async move { reqwest::get(&url).await });\n}\n```\n\n### Graceful Shutdown\n\n```rust\nlet (shutdown_tx, shutdown_rx) = tokio::sync::oneshot::channel();\n\ntokio::spawn(async move {\n    tokio::signal::ctrl_c().await.unwrap();\n    shutdown_tx.send(()).unwrap();\n});\n\naxum::serve(listener, app)\n    .with_graceful_shutdown(async { shutdown_rx.await.ok(); })\n    .await?;\n```\n\n---\n\n## Testing\n\n### Unit Tests (Pure Logic)\n\n```rust\n#[cfg(test)]\nmod tests {\n    use super::*;"}, {"id": "a5e2e2c9-7475-4004-bae4-74dfcc62b9c9", "document_id": "52295064-5d1d-4d5f-b455-44198c5f3c11", "chunk_index": 2, "text": "#[test]\n    fn test_calculate_shipping() {\n        let order = Order {\n            items: vec![item(10_00, 2), item(25_00, 1)],\n            destination: Address::domestic(),\n        };\n        \n        assert_eq!(calculate_shipping(&order), 5_99);\n    }\n    \n    #[test]\n    fn test_free_shipping_over_threshold() {\n        let order = Order {\n            items: vec![item(50_00, 1)],\n            destination: Address::domestic(),\n        };\n        \n        assert_eq!(calculate_shipping(&order), 0);\n    }\n}\n```\n\n### Integration Tests (With Database)\n\n```rust\n#[sqlx::test(migrations = \"migrations\")]\nasync fn test_create_and_fetch_order(pool: PgPool) {\n    let order = CreateOrder {\n        user_id: \"usr_123\".into(),\n        items: vec![OrderItem { product_id: \"prod_456\".into(), quantity: 2 }],\n    };\n    \n    let created = OrderRepo::create(&pool, &order).await.unwrap();\n    assert!(created.id.starts_with(\"ord_\"));\n    \n    let fetched = OrderRepo::get(&pool, &created.id).await.unwrap();\n    assert_eq!(fetched.user_id, \"usr_123\");\n    assert_eq!(fetched.items.len(), 1);\n}\n```\n\n---\n\n## Performance\n\n### Avoid Unnecessary Allocations\n\n```rust\n// \u274c Bad: allocates a new String\nfn greet(name: &str) -> String {\n    format!(\"Hello, {}!\", name)\n}\n\n// \u2705 Good: accepts any string-like type\nfn process(input: impl AsRef<str>) {\n    let s = input.as_ref();\n    // work with &str\n}\n\n// \u2705 Good: use Cow for conditional ownership\nuse std::borrow::Cow;\n\nfn normalize(input: &str) -> Cow<'_, str> {\n    if input.contains(' ') {\n        Cow::Owned(input.replace(' ', \"_\"))\n    } else {\n        Cow::Borrowed(input)\n    }\n}\n```\n\n### Use `#[inline]` Judiciously\n\nOnly inline small, hot functions. Let the compiler decide for everything else.\n\n### Profile Before Optimizing\n\n```bash\n# CPU profiling\ncargo flamegraph --bin order-service\n\n# Memory profiling  \nDHAT_OUT_FILE=dhat.json cargo run --features dhat-heap\n```\n\n---\n\n## Dependencies\n\n### Approved Crates"}, {"id": "170bdf08-64f9-43b8-9aaf-e278f0353f30", "document_id": "52295064-5d1d-4d5f-b455-44198c5f3c11", "chunk_index": 3, "text": "| Category | Crate | Version Policy |\n|----------|-------|----------------|\n| HTTP server | axum | Latest stable |\n| HTTP client | reqwest | Latest stable |\n| Database | sqlx | Latest stable |\n| Serialization | serde + serde_json | Latest stable |\n| Async runtime | tokio | Latest stable |\n| Error handling | anyhow + thiserror | Latest stable |\n| Logging | tracing | Latest stable |\n| CLI | clap | Latest stable |\n| Time | chrono | Latest stable |\n| UUID | uuid | Latest stable |\n\n### Adding New Dependencies\n\n1. Check if an existing approved crate covers the use case\n2. Evaluate: maintenance activity, security history, compile time impact\n3. Get approval from team lead for new dependencies\n4. Run `cargo audit` after adding"}, {"id": "776ca588-e6fb-47f1-9b04-6898a7571013", "document_id": "57e4e4f6-6c95-4d2e-a24c-800291a50530", "chunk_index": 0, "text": "# Team Topology & Ownership\n\nAcme Engineering is organized around Team Topologies principles with stream-aligned teams, platform teams, and enabling teams.\n\n---\n\n## Stream-Aligned Teams\n\nThese teams own end-to-end business capabilities and deliver value directly to users.\n\n### Orders Team\n\n**Mission:** Own the order lifecycle from cart to delivery  \n**Lead:** Jordan Taylor  \n**Size:** 6 engineers  \n**Stack:** Rust (backend), React (frontend), PostgreSQL, Kafka\n\n**Owns:**\n- Order creation and management API\n- Payment processing integration (Stripe)\n- Order fulfillment workflow\n- Order status notifications\n- Returns and refunds\n\n**Key Metrics:**\n- Order success rate: > 99.5%\n- Checkout latency p99: < 2 seconds\n- Refund processing time: < 24 hours\n\n---\n\n### Search & Discovery Team\n\n**Mission:** Help users find what they're looking for  \n**Lead:** Wei Zhang  \n**Size:** 5 engineers  \n**Stack:** Rust (ranking engine), Elasticsearch, Python (ML models)\n\n**Owns:**\n- Search API and ranking algorithm\n- Product recommendations\n- Browse and filter experience\n- Search analytics and A/B testing\n- Autocomplete and spell correction\n\n**Key Metrics:**\n- Search latency p99: < 500ms\n- Click-through rate on first page: > 35%\n- Zero-results rate: < 5%\n\n---\n\n### User Experience Team\n\n**Mission:** Authentication, profiles, and personalization  \n**Lead:** Rachel Adams  \n**Size:** 4 engineers  \n**Stack:** TypeScript (Next.js), PostgreSQL, Redis\n\n**Owns:**\n- User registration and authentication\n- Profile management\n- Notification preferences\n- Wishlist and saved items\n- Personalization signals\n\n---\n\n## Platform Team\n\n### Infrastructure & Platform\n\n**Mission:** Make it easy and safe for stream-aligned teams to deliver  \n**Lead:** Alex Kim  \n**Size:** 7 engineers  \n**Stack:** Kubernetes, Terraform, Nix, Go, Rust"}, {"id": "2f4e87c1-5fd4-4414-8ceb-61f8065a40e9", "document_id": "57e4e4f6-6c95-4d2e-a24c-800291a50530", "chunk_index": 1, "text": "**Owns:**\n- Kubernetes cluster management\n- CI/CD pipeline (GitHub Actions \u2192 ArgoCD)\n- Service mesh (Istio)\n- Container registry and build systems\n- Developer experience tooling\n- Cost optimization\n\n**SLAs:**\n- CI build time: < 10 minutes for 95th percentile\n- Deployment pipeline: < 30 minutes from merge to production\n- Platform availability: > 99.99%\n\n---\n\n### Data Platform\n\n**Mission:** Enable data-driven decisions across all teams  \n**Lead:** Priya Patel  \n**Size:** 5 engineers  \n**Stack:** Snowflake, dbt, Airflow, Kafka, Flink, Python\n\n**Owns:**\n- Data warehouse (Snowflake)\n- ETL pipelines and data quality\n- Feature store (Feast)\n- ML training and serving infrastructure\n- Data governance and access control\n\n---\n\n## Enabling Teams\n\n### SRE Team\n\n**Mission:** Reliability across all production services  \n**Lead:** Marcus Rivera  \n**Size:** 4 engineers\n\n**Provides:**\n- On-call support and incident management\n- SLO/SLI definitions and monitoring\n- Capacity planning\n- Chaos engineering exercises\n- Reliability consulting for new services\n\n**Engagement model:** SRE reviews all new service launches and provides reliability guidance. Teams with services below SLO get embedded SRE support.\n\n---\n\n### Security Team\n\n**Mission:** Protect our systems and customer data  \n**Lead:** Nadia Hassan  \n**Size:** 3 engineers\n\n**Provides:**\n- Security architecture reviews\n- Penetration testing\n- Vulnerability management\n- Compliance (SOC 2, GDPR)\n- Security training and awareness\n\n---\n\n## Service Ownership Matrix"}, {"id": "3ae00b87-5287-44d0-be03-44f511b3bde9", "document_id": "57e4e4f6-6c95-4d2e-a24c-800291a50530", "chunk_index": 2, "text": "| Service | Owning Team | On-Call | Dependencies |\n|---------|-------------|---------|--------------|\n| Order API | Orders | Orders | Payment, Inventory |\n| Payment Service | Orders | Orders | Stripe API |\n| Search API | Search | Search | Elasticsearch, ML |\n| Ranking Engine | Search | Search | Feature Store |\n| User Service | User Experience | UX | PostgreSQL, Redis |\n| Auth Service | User Experience | UX | Okta, Vault |\n| API Gateway | Platform | Platform | Istio |\n| Data Pipeline | Data Platform | Data | Snowflake, Kafka |\n\n---\n\n## Cross-Team Communication\n\n### Dependency Requests\n\nWhen your team needs something from another team:\n\n1. Open a Jira ticket in the provider team's backlog\n2. Label it `cross-team` with priority and deadline\n3. Attend their sprint planning if it's urgent\n4. For API changes, propose via RFC in #engineering-rfcs\n\n### RFC Process\n\nMajor cross-team changes require an RFC:\n\n1. Author writes RFC using template in `docs/rfcs/`\n2. Post in #engineering-rfcs for async review (1 week)\n3. Schedule review meeting if needed\n4. Approvals required from affected team leads\n5. Decision documented and shared\n\n---\n\n## Team Health\n\nWe measure team health quarterly using these dimensions:\n\n1. **Delivery speed** \u2014 How quickly can the team ship value?\n2. **Quality** \u2014 Error rates, incident frequency, tech debt trend\n3. **Autonomy** \u2014 Can the team deliver without cross-team blockers?\n4. **Learning** \u2014 Is the team growing skills and improving processes?\n5. **Satisfaction** \u2014 Do team members enjoy their work?\n\nResults are discussed in retros and shared with engineering leadership."}, {"id": "851fd500-32b7-4d50-9f81-40e08dec065f", "document_id": "6dc49faf-7443-4970-a68d-cb22c35d0148", "chunk_index": 0, "text": "# Observability Stack\n\nOur observability strategy follows the three pillars: metrics, logs, and traces.\n\n---\n\n## Metrics (Datadog)\n\n### Key Dashboards\n\n| Dashboard | Purpose | Alert Threshold |\n|-----------|---------|-----------------|\n| Service Health | Request rate, error rate, latency | Error rate > 1%, p99 > 2s |\n| Infrastructure | CPU, memory, disk, network | CPU > 80%, memory > 85% |\n| Business KPIs | Orders/min, revenue, conversion | Orders drop > 20% |\n| Kafka | Consumer lag, partition balance | Lag > 10,000 messages |\n| Database | Connections, query latency, replication lag | Connections > 80%, lag > 1s |\n\n### Custom Metrics Convention\n\n```\nacme.<service>.<subsystem>.<metric>\n```\n\nExamples:\n- `acme.order_service.api.request_duration`\n- `acme.order_service.kafka.messages_processed`\n- `acme.order_service.db.query_duration`\n- `acme.order_service.cache.hit_ratio`\n\n### SLOs and Error Budgets\n\n| Service | Availability SLO | Latency SLO (p99) | Error Budget/Month |\n|---------|-------------------|--------------------|--------------------|\n| Order API | 99.95% | 500ms | 21.9 minutes |\n| Payment API | 99.99% | 200ms | 4.3 minutes |\n| Search API | 99.9% | 1000ms | 43.8 minutes |\n| User API | 99.95% | 300ms | 21.9 minutes |\n\nWhen error budget is exhausted, the team must prioritize reliability work over feature development until the budget is replenished.\n\n---\n\n## Logging (Datadog Logs)\n\n### Log Levels\n\n| Level | Usage | Sampled in Prod |\n|-------|-------|-----------------|\n| ERROR | Unexpected failures requiring investigation | 100% |\n| WARN | Degraded behavior, retryable errors | 100% |\n| INFO | Key business events, request lifecycle | 10% |\n| DEBUG | Detailed diagnostic information | 0% (enable via feature flag) |\n\n### Structured Logging Format\n\nAll services must use structured JSON logging:"}, {"id": "e929e91c-9262-48e9-9114-5234f9a4d10c", "document_id": "6dc49faf-7443-4970-a68d-cb22c35d0148", "chunk_index": 1, "text": "```json\n{\n  \"timestamp\": \"2025-10-15T14:30:00.123Z\",\n  \"level\": \"ERROR\",\n  \"service\": \"order-service\",\n  \"trace_id\": \"abc123\",\n  \"span_id\": \"def456\",\n  \"message\": \"Failed to process order\",\n  \"error\": {\n    \"type\": \"PaymentDeclined\",\n    \"message\": \"Card expired\",\n    \"stack_trace\": \"...\"\n  },\n  \"context\": {\n    \"order_id\": \"ord_789\",\n    \"user_id\": \"usr_012\",\n    \"amount_cents\": 5999\n  }\n}\n```\n\n### Rust Logging Setup\n\n```rust\nuse tracing::{info, error, instrument};\nuse tracing_subscriber::{layer::SubscriberExt, util::SubscriberInitExt};\n\n#[instrument(skip(db_pool))]\nasync fn process_order(order_id: &str, db_pool: &PgPool) -> Result<()> {\n    info!(order_id, \"Processing order\");\n    \n    match do_payment(order_id).await {\n        Ok(payment) => {\n            info!(order_id, payment_id = %payment.id, \"Payment succeeded\");\n            Ok(())\n        }\n        Err(e) => {\n            error!(order_id, error = %e, \"Payment failed\");\n            Err(e)\n        }\n    }\n}\n```\n\n---\n\n## Distributed Tracing (Datadog APM)\n\n### Trace Propagation\n\nAll services must propagate trace context via HTTP headers:\n\n```\ntraceparent: 00-abc123-def456-01\ntracestate: dd=t.dm:-1\n```\n\n### Key Traces to Monitor\n\n1. **Order lifecycle:** API \u2192 Validation \u2192 Payment \u2192 Fulfillment \u2192 Notification\n2. **Search pipeline:** Query \u2192 Parse \u2192 Execute \u2192 Rank \u2192 Return\n3. **Data sync:** Connector \u2192 Ingest \u2192 Transform \u2192 Store \u2192 Index\n\n### Trace Sampling\n\n| Environment | Sampling Rate |\n|-------------|---------------|\n| Development | 100% |\n| Staging | 100% |\n| Production | 10% base, 100% for errors, 100% for slow requests (>1s) |\n\n---\n\n## Alerting\n\n### Alert Routing\n\n```yaml\n# PagerDuty escalation policy\n- name: \"P1 Critical\"\n  rules:\n    - notify: on-call-primary\n      delay: 0 minutes\n    - notify: on-call-secondary\n      delay: 5 minutes\n    - notify: engineering-manager\n      delay: 15 minutes"}, {"id": "9e031ccb-0bae-4d6f-bb67-ea3c1b992833", "document_id": "6dc49faf-7443-4970-a68d-cb22c35d0148", "chunk_index": 2, "text": "- name: \"P2 Warning\"\n  rules:\n    - notify: team-slack-channel\n      delay: 0 minutes\n    - notify: on-call-primary\n      delay: 30 minutes\n```\n\n### Alert Hygiene\n\n- Every alert must have a runbook link\n- Alert fatigue review: monthly audit of alert frequency\n- Goal: < 5 actionable pages per on-call shift\n- Noisy alerts are either fixed, tuned, or deleted \u2014 never ignored\n\n---\n\n## Runbook Integration\n\nEvery alert links to a runbook in our engineering handbook. Runbooks must include:\n\n1. **What this alert means** \u2014 plain English description\n2. **Impact** \u2014 what users experience\n3. **Investigation steps** \u2014 ordered diagnostic steps\n4. **Mitigation** \u2014 immediate actions to reduce impact\n5. **Resolution** \u2014 steps to fix the root cause\n6. **Escalation** \u2014 who to page if you can't resolve"}, {"id": "ff214ed6-ab8f-4b79-bcaa-4e62aa9b6aa4", "document_id": "bedca66c-b818-462b-b173-e66d0a6d6e47", "chunk_index": 0, "text": "# Architecture Decision Records\n\n## ADR-001: Microservices over Monolith\n\n**Status:** Accepted  \n**Date:** 2025-06-15  \n**Author:** Sarah Chen, Principal Engineer\n\n### Context\n\nOur monolithic application has grown to 2.3 million lines of code. Build times exceed 45 minutes. Teams are blocking each other on deployments. The blast radius of any change affects the entire system.\n\n### Decision\n\nWe will decompose the monolith into domain-aligned microservices over the next 18 months. Each bounded context will become an independently deployable service.\n\n### Consequences\n\n- Teams can deploy independently with reduced coordination overhead\n- We must invest in service mesh infrastructure (Istio)\n- Cross-service transactions require saga patterns\n- Operational complexity increases significantly \u2014 we need robust observability\n\n---\n\n## ADR-002: Event-Driven Architecture with Kafka\n\n**Status:** Accepted  \n**Date:** 2025-07-22  \n**Author:** Marcus Rivera, Staff Engineer\n\n### Context\n\nSynchronous REST calls between services create tight coupling and cascade failures. The order processing pipeline requires seven service calls in sequence, creating a 3-second latency budget.\n\n### Decision\n\nAdopt Apache Kafka as the backbone for asynchronous inter-service communication. Domain events will be published to topic partitions. Services consume events and maintain local read models.\n\n### Consequences\n\n- Decoupled services improve resilience \u2014 one service failure doesn't cascade\n- Eventual consistency requires careful handling in the UI layer\n- We need schema registry (Confluent) for event contract management\n- Engineers must understand event sourcing patterns\n\n---\n\n## ADR-003: PostgreSQL as Primary Datastore\n\n**Status:** Accepted  \n**Date:** 2025-08-10  \n**Author:** Priya Patel, Database Engineering Lead\n\n### Context"}, {"id": "e86ebbc8-d43c-41bb-89a5-6cac4ab67dc3", "document_id": "bedca66c-b818-462b-b173-e66d0a6d6e47", "chunk_index": 1, "text": "We evaluated PostgreSQL, CockroachDB, and DynamoDB for our primary datastore needs. Our workload is 70% reads, 30% writes with complex query patterns including full-text search and JSON operations.\n\n### Decision\n\nPostgreSQL 16 with logical replication for read replicas. We will use JSONB columns for flexible metadata and pg_trgm for fuzzy text search.\n\n### Consequences\n\n- Mature ecosystem with excellent tooling\n- Logical replication enables zero-downtime migrations\n- We must manage connection pooling carefully (PgBouncer)\n- Vertical scaling limits require careful schema design and partitioning strategy\n\n---\n\n## ADR-004: Rust for Performance-Critical Services\n\n**Status:** Accepted  \n**Date:** 2025-09-01  \n**Author:** Alex Kim, Platform Team Lead\n\n### Context\n\nOur data ingestion pipeline processes 50,000 events per second. The current Python implementation consumes 32GB of RAM and occasionally drops events during traffic spikes. Garbage collection pauses cause latency outliers at p99.\n\n### Decision\n\nRewrite the ingestion pipeline and matching engine in Rust. Use Tokio for async I/O and zero-copy deserialization with serde.\n\n### Consequences\n\n- 10x throughput improvement with 4x less memory\n- Compilation guarantees eliminate entire classes of runtime errors\n- Smaller talent pool \u2014 must invest in Rust training\n- Build times are longer than Go but the safety guarantees are worth it"}, {"id": "6a83d799-441f-490c-8d83-bbd6c196092a", "document_id": "c7d91b56-13af-44f2-ae81-f011ba792853", "chunk_index": 0, "text": "# Engineering Onboarding Guide\n\nWelcome to Acme Engineering! This guide will get you productive in your first two weeks.\n\n---\n\n## Week 1: Setup & Orientation\n\n### Day 1: Environment Setup\n\n1. **Laptop configuration**\n   - Install Nix package manager: `curl -L https://nixos.org/nix/install | sh`\n   - Clone the dev environment: `git clone git@github.com:acme/dev-env.git`\n   - Run setup: `cd dev-env && make setup`\n   - This installs: Rust toolchain, Node.js 20, Python 3.12, Docker, kubectl\n\n2. **Access provisioning**\n   - GitHub: Request access to `acme` org in #it-helpdesk\n   - AWS: SSO login configured via Okta \u2014 use `aws sso login --profile acme-dev`\n   - Kubernetes: Kubeconfig distributed via `dev-env` setup\n   - Datadog: Request access from your team lead\n   - PagerDuty: Added to your team's rotation after Week 2\n\n3. **First PR**\n   - Add yourself to `team-directory.yaml`\n   - This verifies your Git, CI, and code review setup works end-to-end\n\n### Day 2-3: Architecture Overview\n\n- Watch the \"System Architecture\" recording (45 min) on Loom\n- Read ADR-001 through ADR-004 in the architecture-decisions doc\n- Shadow a senior engineer during their daily work\n- Tour of the Grafana dashboards and key metrics\n\n### Day 4-5: First Feature\n\n- Pick a \"good first issue\" from your team's backlog\n- Pair with your onboarding buddy\n- Ship your first feature (however small!) by end of Week 1\n\n---\n\n## Week 2: Deep Dive\n\n### Team-Specific Training\n\nEach team has a specialized onboarding track:\n\n**Platform Team:**\n- Kubernetes cluster architecture\n- CI/CD pipeline (GitHub Actions \u2192 ArgoCD)\n- Infrastructure-as-code (Terraform + Nix)\n- On-call shadowing\n\n**Product Team:**\n- Feature flag system (LaunchDarkly)\n- A/B testing framework\n- Frontend architecture (React + Next.js)\n- User analytics pipeline\n\n**Data Team:**\n- Data warehouse architecture (Snowflake)\n- ETL pipelines (dbt + Airflow)\n- Data contracts and schema evolution\n- ML model serving infrastructure\n\n---"}, {"id": "a51e2161-204a-4ce8-88f1-75856aa6ad2d", "document_id": "c7d91b56-13af-44f2-ae81-f011ba792853", "chunk_index": 1, "text": "## Development Workflow\n\n### Branch Strategy\n\nWe use trunk-based development:\n\n1. Create a short-lived feature branch from `main`\n2. Keep branches under 400 lines changed\n3. Open PR with description template\n4. Require 1 approval (2 for infrastructure changes)\n5. CI must pass: tests, linting, type checking, security scan\n6. Squash merge to `main`\n7. Auto-deploy to staging, manual promotion to production\n\n### Code Review Guidelines\n\n- Review within 4 business hours\n- Focus on: correctness, readability, test coverage, security\n- Use \"nitpick:\" prefix for style suggestions\n- Block only for: bugs, security issues, missing tests, breaking changes\n\n### Testing Requirements\n\n- Unit test coverage: minimum 80% for new code\n- Integration tests for all API endpoints\n- E2E tests for critical user flows\n- Performance benchmarks for hot paths\n\n---\n\n## Communication\n\n### Meetings\n\n- **Daily standup:** 9:15 AM, 15 minutes max\n- **Sprint planning:** Monday 10 AM (bi-weekly)\n- **Retro:** Friday 3 PM (bi-weekly)\n- **Tech talks:** Thursday 2 PM (weekly, rotating presenter)\n- **All-hands:** Tuesday 11 AM (monthly)\n\n### Slack Channels\n\n- `#engineering` \u2014 General engineering discussion\n- `#incidents` \u2014 Active incident coordination\n- `#deploys` \u2014 Deployment notifications\n- `#code-review` \u2014 PR review requests\n- `#til` \u2014 Today I Learned (share interesting discoveries)\n- `#random` \u2014 Water cooler chat\n\n---\n\n## Key Contacts\n\n| Role | Person | Slack |\n|------|--------|-------|\n| VP Engineering | Dana Morrison | @dana |\n| Platform Lead | Alex Kim | @alexk |\n| Product Lead | Jordan Taylor | @jtaylor |\n| Data Lead | Priya Patel | @priya |\n| SRE Lead | Marcus Rivera | @marcus |\n| HR / People Ops | Sam Williams | @samw |"}, {"id": "35d3ddf4-aaf8-4bb9-8a36-9b11836eed3c", "document_id": "c8b7a0ee-7694-4e2e-be9d-d98828af90b7", "chunk_index": 0, "text": "# Data Platform Architecture\n\nOur data platform enables analytics, machine learning, and real-time data processing at scale.\n\n---\n\n## Architecture Overview\n\n```\nSources \u2192 Ingestion \u2192 Processing \u2192 Storage \u2192 Serving\n                                                 \u2193\n                                           ML Training\n                                                 \u2193\n                                           Model Serving\n```\n\n---\n\n## Data Ingestion\n\n### Batch Ingestion (dbt + Airflow)\n\nHourly and daily batch jobs extract data from operational databases into Snowflake:\n\n```sql\n-- dbt model: orders_enriched\nSELECT\n    o.id,\n    o.created_at,\n    o.total_amount_cents,\n    o.status,\n    u.segment,\n    u.lifetime_value_cents,\n    p.category,\n    p.brand\nFROM {{ ref('stg_orders') }} o\nJOIN {{ ref('stg_users') }} u ON o.user_id = u.id\nJOIN {{ ref('stg_products') }} p ON o.product_id = p.id\nWHERE o.created_at >= '{{ var(\"start_date\") }}'\n```\n\n### Stream Processing (Kafka + Flink)\n\nReal-time events flow through Kafka to Apache Flink for:\n- Fraud detection (sub-second decisions)\n- Real-time inventory updates\n- Live conversion funnel analytics\n- Personalization signal aggregation\n\n```java\nDataStream<OrderEvent> orders = env\n    .fromSource(kafkaSource, WatermarkStrategy.forMonotonousTimestamps(), \"orders\")\n    .keyBy(OrderEvent::getUserId)\n    .window(TumblingEventTimeWindows.of(Time.minutes(5)))\n    .aggregate(new OrderAggregator());\n```\n\n---\n\n## Data Warehouse (Snowflake)\n\n### Schema Organization\n\n| Schema | Purpose | Refresh |\n|--------|---------|---------|\n| `raw` | Unmodified source data | Continuous |\n| `staging` | Cleaned, typed, deduplicated | Hourly |\n| `marts` | Business-ready aggregations | Hourly |\n| `ml_features` | Feature store for ML models | Daily |\n\n### Data Contracts\n\nEvery table has a contract defined in YAML:"}, {"id": "e36714b6-dbe9-4b58-bfbe-54c36559d9e1", "document_id": "c8b7a0ee-7694-4e2e-be9d-d98828af90b7", "chunk_index": 1, "text": "```yaml\nmodel:\n  name: orders_enriched\n  description: \"Orders with user and product dimensions\"\n  columns:\n    - name: id\n      type: string\n      tests: [not_null, unique]\n    - name: total_amount_cents\n      type: integer\n      tests: [not_null, positive]\n    - name: created_at\n      type: timestamp\n      tests: [not_null]\n  freshness:\n    warn_after: 2 hours\n    error_after: 4 hours\n```\n\nBreaking contract changes require:\n1. ADR with justification\n2. 2-week migration window\n3. Downstream consumer notification\n4. Backward-compatible transition period\n\n---\n\n## Machine Learning Infrastructure\n\n### Feature Store\n\nWe use Feast for online and offline feature serving:\n\n```python\nfrom feast import FeatureStore\n\nstore = FeatureStore(\"feature_repo/\")\n\n# Online serving (low latency)\nfeatures = store.get_online_features(\n    features=[\"user_features:lifetime_value\", \"user_features:order_count\"],\n    entity_rows=[{\"user_id\": \"usr_123\"}]\n)\n\n# Offline training (batch)\ntraining_df = store.get_historical_features(\n    entity_df=entity_df,\n    features=[\"user_features:lifetime_value\", \"product_features:category\"]\n)\n```\n\n### Model Training Pipeline\n\n1. **Data preparation:** dbt transforms \u2192 Feast features\n2. **Training:** SageMaker training jobs with experiment tracking (MLflow)\n3. **Evaluation:** Automated evaluation against holdout set\n4. **Registry:** Model registered in MLflow with metrics and artifacts\n5. **Deployment:** Canary deployment via SageMaker endpoints\n\n### Model Serving\n\n| Use Case | Serving Pattern | Latency Target |\n|----------|----------------|----------------|\n| Fraud detection | Real-time (gRPC) | < 50ms |\n| Recommendations | Near-real-time (REST) | < 200ms |\n| Forecasting | Batch (Airflow) | N/A |\n| Search ranking | Real-time (REST) | < 100ms |\n\n---\n\n## Data Quality\n\n### Automated Checks"}, {"id": "0e515f1f-3905-4d2d-a53a-cee9b43a68d9", "document_id": "c8b7a0ee-7694-4e2e-be9d-d98828af90b7", "chunk_index": 2, "text": "- **Schema validation:** Great Expectations checks on every pipeline run\n- **Freshness monitoring:** Alerts if data is stale beyond SLA\n- **Volume anomaly detection:** Statistical checks for unexpected drops/spikes\n- **Cross-source reconciliation:** Daily counts compared across systems\n\n### Data Lineage\n\nWe track data lineage automatically via dbt and Airflow:\n\n```\nraw.stripe_charges\n    \u2192 staging.stg_payments\n        \u2192 marts.revenue_by_day\n            \u2192 Dashboard: \"Revenue Overview\"\n            \u2192 ML Model: \"Churn Prediction\"\n```\n\nLineage helps answer: \"If this source table changes, what dashboards and models are affected?\"\n\n---\n\n## Access Control\n\n| Role | Access | Approval |\n|------|--------|----------|\n| Analyst | Read marts schemas | Team lead |\n| Data Engineer | Read/write all schemas | Data lead |\n| ML Engineer | Read staging + ml_features | Data lead |\n| Service Account | Specific tables via policy | Data lead + Security |\n\nPII data requires additional approval from the Privacy team and is accessed via tokenized views only."}, {"id": "3ba5d355-9714-4398-b5e5-7470701aee15", "document_id": "c9819917-3409-4c5b-8163-7d51a4925c02", "chunk_index": 0, "text": "# Security Practices\n\nSecurity is everyone's responsibility. These practices are mandatory for all engineers.\n\n---\n\n## Authentication & Authorization\n\n### Identity Provider\n\nAll human access goes through Okta SSO with mandatory MFA:\n\n- **Primary factor:** Okta Verify push notification\n- **Backup factors:** TOTP codes, hardware security keys (YubiKey)\n- **Session duration:** 12 hours for standard access, 1 hour for admin access\n- **Conditional access:** VPN required for production access from non-corporate networks\n\n### Service Identity\n\nServices authenticate using short-lived certificates managed by the Istio service mesh:\n\n1. Each pod gets a unique SPIFFE identity: `spiffe://acme.com/ns/production/sa/order-service`\n2. Certificates auto-rotate every 24 hours\n3. mTLS enforced for all inter-service communication\n4. Authorization policies define which services can communicate\n\n```yaml\napiVersion: security.istio.io/v1\nkind: AuthorizationPolicy\nmetadata:\n  name: order-service-policy\nspec:\n  selector:\n    matchLabels:\n      app: order-service\n  rules:\n    - from:\n        - source:\n            principals: [\"cluster.local/ns/production/sa/api-gateway\"]\n      to:\n        - operation:\n            methods: [\"GET\", \"POST\"]\n            paths: [\"/v1/orders*\"]\n```\n\n---\n\n## Secrets Management\n\n### HashiCorp Vault\n\nAll secrets are stored in Vault and injected at runtime:\n\n```bash\n# Application reads secrets from Vault agent sidecar\nvault kv get secret/order-service/database\n```\n\n### Secret Rotation\n\n| Secret Type | Rotation Period | Automated |\n|-------------|-----------------|-----------|\n| Database passwords | 30 days | Yes |\n| API keys | 90 days | Yes |\n| TLS certificates | 24 hours | Yes (cert-manager) |\n| Encryption keys | 365 days | Yes (AWS KMS) |\n\n### Never Do This"}, {"id": "346eb025-99cf-4acb-bcb5-8f596109938b", "document_id": "c9819917-3409-4c5b-8163-7d51a4925c02", "chunk_index": 1, "text": "- \u274c Commit secrets to Git (even in private repos)\n- \u274c Store secrets in environment variables in CI\n- \u274c Share secrets via Slack or email\n- \u274c Use long-lived API keys for service accounts\n- \u274c Hardcode credentials in application code\n\n---\n\n## Data Protection\n\n### Encryption\n\n| Layer | Method | Key Management |\n|-------|--------|----------------|\n| At rest | AES-256 | AWS KMS |\n| In transit | TLS 1.3 | cert-manager |\n| Application-level | AES-256-GCM | Vault Transit |\n\n### PII Handling\n\nPersonal Identifiable Information requires special handling:\n\n1. **Classification:** All data fields must be classified (PII, Sensitive, Internal, Public)\n2. **Minimization:** Only collect PII that is strictly necessary\n3. **Tokenization:** PII is tokenized before storage; original values in a separate token vault\n4. **Retention:** PII deleted after retention period (default: 2 years, configurable per regulation)\n5. **Access logging:** All PII access is logged and auditable\n\n### GDPR Compliance\n\n- Right to erasure: automated deletion pipeline triggered via admin API\n- Data portability: export endpoint returns all user data in JSON format\n- Consent tracking: granular consent stored per data processing purpose\n- Cross-border transfers: EU data stays in eu-west-1 region\n\n---\n\n## Vulnerability Management\n\n### Dependency Scanning\n\n```bash\n# Run in CI on every PR\ncargo audit                    # Rust advisory database\nsnyk test --all-projects      # Transitive dependency scan\ntrivy image acme/order:latest  # Container vulnerability scan\n```\n\n### Severity Response Times\n\n| Severity | Response | Fix Deadline |\n|----------|----------|-------------|\n| Critical (CVSS 9.0+) | Immediate page | 24 hours |\n| High (CVSS 7.0-8.9) | Same-day triage | 1 week |\n| Medium (CVSS 4.0-6.9) | Sprint planning | 1 sprint |\n| Low (CVSS 0.1-3.9) | Backlog | Next quarter |\n\n### Penetration Testing"}, {"id": "116c0bfe-3180-45db-be7e-e5f28fd1ed55", "document_id": "c9819917-3409-4c5b-8163-7d51a4925c02", "chunk_index": 2, "text": "- Annual third-party pentest of all external-facing services\n- Quarterly internal red team exercises\n- Bug bounty program (HackerOne) with payouts up to $10,000\n\n---\n\n## Incident Security Response\n\nIf you suspect a security incident:\n\n1. **Do not try to fix it yourself** \u2014 contain and escalate\n2. Notify `#security-incidents` Slack channel immediately\n3. Page the security on-call: `pd trigger security`\n4. Preserve evidence \u2014 do not delete logs or modify systems\n5. Document everything with timestamps\n\n### Security Incident Classification\n\n| Type | Example | Escalation |\n|------|---------|------------|\n| Data breach | PII exposed externally | CEO, Legal, CISO |\n| Unauthorized access | Suspicious login patterns | Security team |\n| Malware | Compromised dependency | Security + Platform |\n| Social engineering | Phishing targeting engineers | Security + HR |"}, {"id": "71241ffb-8bd6-44b9-911c-a1d9b9e917f0", "document_id": "e99d4d1b-b29f-4be3-a36e-63ef91be7dc6", "chunk_index": 0, "text": "# Post-Mortem: Order Processing Outage \u2014 October 3, 2025\n\n**Severity:** SEV-1  \n**Duration:** 47 minutes  \n**Incident Commander:** Marcus Rivera  \n**Author:** Sarah Chen\n\n---\n\n## Summary\n\nOn October 3, 2025 at 14:23 UTC, the order processing pipeline stopped processing new orders for 47 minutes. During this window, approximately 12,400 orders were queued but not processed, affecting 8,200 unique customers. No orders were lost \u2014 all queued orders were processed after recovery.\n\nRevenue impact was estimated at $340,000 in delayed transactions (all recovered).\n\n---\n\n## Timeline\n\n| Time (UTC) | Event |\n|------------|-------|\n| 14:23 | Kafka consumer lag alert fires for order-processor consumer group |\n| 14:25 | On-call SRE (Marcus) acknowledges alert |\n| 14:28 | Marcus identifies that all order-processor pods are in CrashLoopBackOff |\n| 14:30 | SEV-1 declared, incident channel created |\n| 14:32 | Application logs show: `panicked at 'Failed to deserialize OrderEvent: missing field \"shipping_method\"'` |\n| 14:35 | Root cause identified: a deployment to the Order API 20 minutes earlier added a required `shipping_method` field to OrderEvent |\n| 14:38 | Two options identified: (1) rollback Order API, (2) deploy fix to order-processor |\n| 14:42 | Decision: rollback Order API as it's faster |\n| 14:45 | Order API rolled back to previous version |\n| 14:50 | Order-processor pods restart and begin draining the backlog |\n| 15:10 | All queued orders processed, consumer lag returns to zero |\n| 15:15 | SEV-1 resolved, incident channel archived |\n\n---\n\n## Root Cause Analysis\n\n### 5 Whys\n\n1. **Why did the order-processor crash?**  \n   Because it couldn't deserialize the OrderEvent \u2014 the `shipping_method` field was missing.\n\n2. **Why was the field missing?**  \n   Because the Order API was deployed with a schema change that added `shipping_method` as a required field in OrderEvent, but the order-processor hadn't been updated to handle it."}, {"id": "02aa4819-363e-4a37-b87b-f763e60f4d90", "document_id": "e99d4d1b-b29f-4be3-a36e-63ef91be7dc6", "chunk_index": 1, "text": "3. **Why wasn't the order-processor updated first?**  \n   Because the schema change wasn't flagged as a breaking change during code review. The PR description didn't mention the Kafka event schema change.\n\n4. **Why wasn't the breaking change caught in CI?**  \n   Because we don't have automated schema compatibility checks for Kafka events. Our contract tests only cover REST APIs.\n\n5. **Why don't we have schema compatibility checks for events?**  \n   Because when we adopted Kafka, we decided to use plain JSON serialization for simplicity. We haven't implemented a schema registry yet despite it being on the roadmap for Q3.\n\n### Contributing Factors\n\n- The Order API and order-processor are owned by the same team, creating a false sense of safety\n- The Kafka event schema was only documented in code comments, not in a formal contract\n- The deploy happened right before a meeting, reducing the deployer's monitoring window\n\n---\n\n## What Went Well\n\n1. **Fast detection** \u2014 Kafka consumer lag alert fired within 2 minutes\n2. **Fast response** \u2014 On-call acknowledged and triaged within 10 minutes\n3. **Clear runbook** \u2014 The \"Kafka Consumer Lag\" runbook guided investigation\n4. **No data loss** \u2014 Kafka retention ensured all events were preserved\n5. **Clean rollback** \u2014 ArgoCD rollback was smooth and took < 5 minutes\n\n---\n\n## What Could Be Improved\n\n1. **No schema validation for Kafka events** \u2014 REST contracts are tested, but event contracts are not\n2. **PR template doesn't prompt for event schema changes** \u2014 reviewers didn't think to check\n3. **No canary for event consumers** \u2014 the crash affected 100% of consumer pods immediately\n4. **Deploy monitoring window too short** \u2014 deployer didn't observe consumer health after deploy\n\n---\n\n## Action Items"}, {"id": "6f5f60d0-6443-4bd4-96f3-5d6278e4c758", "document_id": "e99d4d1b-b29f-4be3-a36e-63ef91be7dc6", "chunk_index": 2, "text": "| # | Action | Owner | Due | Status |\n|---|--------|-------|-----|--------|\n| 1 | Implement Confluent Schema Registry for all Kafka topics | Alex Kim | 2025-11-01 | In Progress |\n| 2 | Add \"Event Schema Changes\" checkbox to PR template | Jordan Taylor | 2025-10-10 | Done |\n| 3 | Create CI check for backward-compatible schema evolution | Sarah Chen | 2025-11-15 | Not Started |\n| 4 | Implement consumer canary deployment strategy | Marcus Rivera | 2025-12-01 | Not Started |\n| 5 | Add 15-minute deploy monitoring requirement to deployment checklist | Jordan Taylor | 2025-10-07 | Done |\n| 6 | Document all Kafka event schemas in schema registry format | Orders Team | 2025-10-31 | In Progress |\n\n---\n\n## Lessons Learned\n\n> \"Making a field required in a shared event is a breaking change, even when you own both the producer and consumer. Always assume consumers deploy on their own schedule.\"\n\nThis incident reinforced the importance of backward-compatible schema evolution. The Kafka ecosystem provides tools for this (Schema Registry with BACKWARD compatibility mode), and we've deprioritized adopting them for too long.\n\n**Rule going forward:** All Kafka event schema changes must be backward-compatible. New required fields must have defaults. Field removal requires a 2-sprint deprecation window."}]}